{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "YOJvphymrXVe",
      "metadata": {
        "id": "YOJvphymrXVe"
      },
      "outputs": [],
      "source": [
        "#!apt-get update\n",
        "#!apt-get install -y swig python3-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c0b813e7",
      "metadata": {
        "id": "c0b813e7"
      },
      "outputs": [],
      "source": [
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36960300",
      "metadata": {
        "id": "36960300"
      },
      "source": [
        "## PPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4dc4177d",
      "metadata": {
        "id": "4dc4177d"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from torch import multiprocessing\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Information\n",
        "from IPython.display import clear_output, display\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import OneHotCategorical\n",
        "\n",
        "# TorchRL\n",
        "from torchrl.envs.transforms import (\n",
        "    TransformedEnv, Compose, ToTensorImage, ObservationNorm, StepCounter, DoubleToFloat, GrayScale, CatFrames, UnsqueezeTransform\n",
        "    )\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "from torchrl.modules import ProbabilisticActor, SafeModule, ValueOperator, TanhNormal\n",
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "\n",
        "# Environment\n",
        "from torchrl.envs.libs.gym import GymEnv\n",
        "\n",
        "# Other\n",
        "import uuid\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7882c92",
      "metadata": {
        "id": "c7882c92"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd75e3ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd75e3ec",
        "outputId": "d04d01c7-b590-4c2b-f424-c0d3526bad5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = (\n",
        "    torch.device(0)\n",
        "    if torch.cuda.is_available() and not is_fork\n",
        "    else torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Collector hyper-parameters\n",
        "frames_per_batch = 1024 # number of frames collected per batch\n",
        "num_iterations = 256 # number of batches\n",
        "total_frames = num_iterations * frames_per_batch  # total number of frames to collect\n",
        "\n",
        "# PPO hyper-parameters\n",
        "sub_batch_size = 128  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
        "num_epochs = 8 # optimization steps per batch of data collected\n",
        "learning_rate = 2e-4 # learning rate for the optimizer\n",
        "\n",
        "# Checkpoint saving parameters\n",
        "checkpoint_interval = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af235e9",
      "metadata": {
        "id": "5af235e9"
      },
      "source": [
        "### Creating the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b586b0b",
      "metadata": {
        "id": "6b586b0b"
      },
      "outputs": [],
      "source": [
        "cont = False  # whether to use continuous actions\n",
        "\n",
        "base_env = GymEnv(\"CarRacing-v3\", continuous=cont, render_mode=\"rgb_array\", device=device)\n",
        "\n",
        "# Compose them into a TransformedEnv\n",
        "env = TransformedEnv(base_env,\n",
        "    Compose(\n",
        "        DoubleToFloat(),\n",
        "        ToTensorImage(),\n",
        "        GrayScale(),\n",
        "        UnsqueezeTransform(-4),\n",
        "        CatFrames(dim=-3, N=4),\n",
        "        ObservationNorm(in_keys=[\"pixels\"]),\n",
        "        StepCounter()\n",
        "    )\n",
        ")\n",
        "\n",
        "# Normalize observations\n",
        "env.transform[-2].init_stats(num_iter=256, reduce_dim=0, cat_dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b01750",
      "metadata": {
        "id": "94b01750"
      },
      "source": [
        "### Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d1b51dd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b51dd4",
        "outputId": "5b960600-886e-4f6b-e863-5678cdf6b830"
      },
      "outputs": [],
      "source": [
        "class CarRacingCritic(nn.Module):\n",
        "    def __init__(self, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "        super().__init__()\n",
        "        # Input has 4 channels (stacked grayscale frames)\n",
        "        self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "        H, W = img_size\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, n_frames, H, W, device=device)\n",
        "            o = self.conv1(dummy)\n",
        "            o = self.conv2(o)\n",
        "            o = self.conv3(o)\n",
        "            self.flatten_size = o.view(1, -1).shape[1]\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "        self.value_head = nn.Linear(512, 1, device=device)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        if obs.dim() == 3:\n",
        "            obs = obs.unsqueeze(0)\n",
        "        x = torch.relu(self.conv1(obs))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.value_head(x)\n",
        "\n",
        "if (cont):\n",
        "    \"\"\" For continuous actions: \"\"\"\n",
        "    class CarRacingContinuous(nn.Module):\n",
        "        def __init__(self, n_actions: int, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "            super().__init__()\n",
        "            # Input has 4 channels (stacked grayscale frames)\n",
        "            self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "            H, W = img_size\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, n_frames, H, W, device=device)\n",
        "                o = self.conv1(dummy)\n",
        "                o = self.conv2(o)\n",
        "                o = self.conv3(o)\n",
        "                self.flatten_size = o.view(1, -1).shape[1]\n",
        "            self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "\n",
        "            # For continuous actions: we output a 3-element mean vector (steer, gas, brake)\n",
        "            self.action_mean = nn.Linear(512, n_actions, device=device)\n",
        "            # and a log standard deviation\n",
        "            self.log_std = nn.Parameter(torch.zeros(n_actions, device=device))\n",
        "    \n",
        "        def forward(self, obs: torch.Tensor):\n",
        "            if obs.dim() == 3:\n",
        "                obs = obs.unsqueeze(0)\n",
        "            x = torch.relu(self.conv1(obs))\n",
        "            x = torch.relu(self.conv2(x))\n",
        "            x = torch.relu(self.conv3(x))\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "            x = torch.relu(self.fc1(x))\n",
        "            mean = self.action_mean(x)        # → [B, 3]\n",
        "            # expand log_std to [B, 3] automatically via broadcasting:\n",
        "            log_std = self.log_std.unsqueeze(0).expand_as(mean)\n",
        "            return mean, log_std              # → two outputs: mean and log_std\n",
        "\n",
        "    backbone_net = SafeModule(\n",
        "        module=CarRacingContinuous(env.action_spec.shape.numel()),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"loc\", \"scale\"],\n",
        "    )\n",
        "\n",
        "    actor = ProbabilisticActor(\n",
        "        module=backbone_net,\n",
        "        spec=env.action_spec,\n",
        "        in_keys=[\"loc\", \"scale\"],\n",
        "        distribution_class=TanhNormal,\n",
        "        out_keys=[\"action\"],\n",
        "        return_log_prob=True,\n",
        "    )\n",
        "\n",
        "    critic_net = SafeModule(\n",
        "        module=CarRacingCritic(),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "    critic = ValueOperator(\n",
        "        module=critic_net,\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "\n",
        "else:\n",
        "    \"\"\" For discrete actions: \"\"\"\n",
        "    class CarRacingDiscrete(nn.Module):\n",
        "        def __init__(self, n_actions: int, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "            # Dynamically compute flatten_size so we never hard‐code incorrectly:\n",
        "            H, W = img_size\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, n_frames, H, W, device=device)  # [1, 3, 96, 96]\n",
        "                o = self.conv1(dummy)\n",
        "                o = self.conv2(o)\n",
        "                o = self.conv3(o)\n",
        "                self.flatten_size = o.view(1, -1).shape[1]  # e.g. 4096\n",
        "\n",
        "            self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "            self.logits = nn.Linear(512, n_actions, device=device)\n",
        "\n",
        "        def forward(self, obs: torch.Tensor):\n",
        "            # If obs has shape [3, 96, 96], unsqueeze so it becomes [1, 3, 96, 96]:\n",
        "            if obs.dim() == 3:                     # no batch dimension\n",
        "                obs = obs.unsqueeze(0)             # now [1, 3, 96, 96]\n",
        "\n",
        "            # By this point, obs must be [B, 3, 96, 96]:\n",
        "            x = torch.relu(self.conv1(obs))        # → [B, 32, 23, 23]\n",
        "            x = torch.relu(self.conv2(x))          # → [B, 64, 10, 10]\n",
        "            x = torch.relu(self.conv3(x))          # → [B, 64,  8,  8]\n",
        "            x = x.reshape(x.shape[0], -1)             # → [B, 4096]  (because flatten_size = 4096)\n",
        "            x = torch.relu(self.fc1(x))            # → [B, 512]\n",
        "            return self.logits(x)                  # → [B, n_actions]\n",
        "\n",
        "    backbone_net = SafeModule(\n",
        "        module=CarRacingDiscrete(env.action_spec.shape.numel()),\n",
        "        in_keys=[\"pixels\"],     # expects obs under key \"pixels\"\n",
        "        out_keys=[\"logits\"],    # produces a \"logits\" tensor\n",
        "    )\n",
        "\n",
        "    actor = ProbabilisticActor(\n",
        "        module=backbone_net,\n",
        "        spec=env.action_spec,               # DiscreteTensorSpec\n",
        "        in_keys=[\"logits\"],                 # read logits from that key\n",
        "        distribution_class=OneHotCategorical,     # TorchRL’s one-hot categorical: samples a one-hot vector of size n_actions\n",
        "        out_keys=[\"action\"],                # writes a one-hot action into \"action\"\n",
        "        return_log_prob=True,               # store \"log_prob\" in the tensordict\n",
        "    )\n",
        "\n",
        "    critic_net = SafeModule(\n",
        "        module=CarRacingCritic(),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "    critic = ValueOperator(\n",
        "        module=critic_net,\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a59bd47",
      "metadata": {
        "id": "9a59bd47"
      },
      "source": [
        "### Data Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f37e84c1",
      "metadata": {
        "id": "f37e84c1"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        "    batch_size=sub_batch_size,\n",
        ")\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    device=device,\n",
        "    split_trajs=False,\n",
        ")\n",
        "\n",
        "gae_module = GAE(\n",
        "    value_network=critic,\n",
        "    gamma=0.99,\n",
        "    lmbda=0.95,\n",
        ")\n",
        "\n",
        "def get_entropy_coef(iteration, total_iters):\n",
        "    if iteration < 0.3 * total_iters:\n",
        "        return 0.03\n",
        "    elif iteration < 0.6 * total_iters:\n",
        "        return 0.015\n",
        "    else:\n",
        "        return 0.005\n",
        "\n",
        "ppo_loss = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=0.2,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        "    \n",
        "    #normalize_advantage=True,\n",
        "\n",
        "    entropy_coef=get_entropy_coef(0, num_iterations),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(ppo_loss.parameters(), lr=2e-4)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ChainedScheduler([\n",
        "    torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-2, total_iters=int(0.1 * num_iterations)), # First 10% of the training\n",
        "    torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(0.9 * num_iterations), eta_min=1e-8), # Last 90% of the training\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375cea7d",
      "metadata": {
        "id": "375cea7d"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cZeNveQfv-Tk",
      "metadata": {
        "id": "cZeNveQfv-Tk"
      },
      "outputs": [],
      "source": [
        "def plot(logs):\n",
        "    # Update plot data\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Rebuild the figure from scratch\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    axes = axes.flatten()\n",
        "    titles = [\n",
        "        \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
        "        #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
        "    ]\n",
        "    data = [\n",
        "        (logs[\"train_reward\"],   \"blue\"),\n",
        "        (logs[\"eval_reward\"],    \"green\"),\n",
        "        #(logs[\"train_steps_min\"],    \"red\"),\n",
        "        #(logs[\"train_steps_max\"],     \"orange\"),\n",
        "    ]\n",
        "\n",
        "    for ax, title, (y, color) in zip(axes, titles, data):\n",
        "        ax.plot(y, color=color)\n",
        "        ax.set_title(title)\n",
        "        ax.relim()\n",
        "        ax.autoscale_view()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the new figure\n",
        "    display(fig)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5f654737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "5f654737",
        "outputId": "4f1d13c9-65f1-4525-90dc-b8256cc203a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training :   0%|          | 0/256 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 1 is out of bounds for axis 0 with size 1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m runDir = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid.uuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m os.mkdir(runDir)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtd\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Clip rewards for stability in discrete action spaces\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:327\u001b[39m, in \u001b[36mDataCollectorBase.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[TensorDictBase]:\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterator()\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mself\u001b[39m.shutdown()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1206\u001b[39m, in \u001b[36mSyncDataCollector.iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._frames < \u001b[38;5;28mself\u001b[39m.total_frames:\n\u001b[32m   1205\u001b[39m     \u001b[38;5;28mself\u001b[39m._iter += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     tensordict_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tensordict_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1208\u001b[39m         \u001b[38;5;66;03m# if a replay buffer is passed and self.extend_buffer=False, there is no tensordict_out\u001b[39;00m\n\u001b[32m   1209\u001b[39m         \u001b[38;5;66;03m#  frames are updated within the rollout function\u001b[39;00m\n\u001b[32m   1210\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\_utils.py:601\u001b[39m, in \u001b[36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch._C._distributed_rpc.PyRRef):\n\u001b[32m    600\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mself\u001b[39m.local_value()\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1458\u001b[39m, in \u001b[36mSyncDataCollector.rollout\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1457\u001b[39m     env_input = \u001b[38;5;28mself\u001b[39m._shuttle\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m env_output, env_next_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[32m   1462\u001b[39m     next_data = env_output.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\envs\\common.py:3585\u001b[39m, in \u001b[36mEnvBase.step_and_maybe_reset\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   3583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensordict.device != \u001b[38;5;28mself\u001b[39m.device:\n\u001b[32m   3584\u001b[39m     tensordict = tensordict.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m3585\u001b[39m tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3586\u001b[39m \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[32m   3587\u001b[39m \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[32m   3588\u001b[39m tensordict_ = \u001b[38;5;28mself\u001b[39m._step_mdp(tensordict)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\envs\\common.py:2031\u001b[39m, in \u001b[36mEnvBase.step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   2028\u001b[39m next_preset = tensordict.get(\u001b[33m\"\u001b[39m\u001b[33mnext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   2030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2031\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2032\u001b[39m     next_tensordict = \u001b[38;5;28mself\u001b[39m._step_proc_data(next_tensordict)\n\u001b[32m   2033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2034\u001b[39m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   2035\u001b[39m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\envs\\transforms\\transforms.py:1129\u001b[39m, in \u001b[36mTransformedEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m   1126\u001b[39m         tensordict_batch_size = \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1129\u001b[39m     next_tensordict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m         \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[32m   1132\u001b[39m         \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[32m   1133\u001b[39m         \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[32m   1134\u001b[39m         next_tensordict.update(\n\u001b[32m   1135\u001b[39m             next_preset.exclude(*next_tensordict.keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m   1136\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\torchrl\\envs\\gym_like.py:451\u001b[39m, in \u001b[36mGymLikeEnv._step\u001b[39m\u001b[34m(self, tensordict)\u001b[39m\n\u001b[32m    449\u001b[39m reward = \u001b[32m0\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.wrapper_frame_skip):\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     step_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     (\n\u001b[32m    453\u001b[39m         obs,\n\u001b[32m    454\u001b[39m         _reward,\n\u001b[32m   (...)\u001b[39m\u001b[32m    458\u001b[39m         info_dict,\n\u001b[32m    459\u001b[39m     ) = \u001b[38;5;28mself\u001b[39m._output_transform(step_result)\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mario\\source\\Bloque Final\\AssessmentAI\\.venv\\Lib\\site-packages\\gymnasium\\envs\\box2d\\car_racing.py:547\u001b[39m, in \u001b[36mCarRacing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    545\u001b[39m     action = action.astype(np.float64)\n\u001b[32m    546\u001b[39m     \u001b[38;5;28mself\u001b[39m.car.steer(-action[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     \u001b[38;5;28mself\u001b[39m.car.gas(\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    548\u001b[39m     \u001b[38;5;28mself\u001b[39m.car.brake(action[\u001b[32m2\u001b[39m])\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 0 with size 1"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(total=num_iterations)\n",
        "pbar.set_description(\"Training \")\n",
        "logs = defaultdict(list)\n",
        "\n",
        "runDir = f'./checkpoints/{uuid.uuid4()}'\n",
        "os.mkdir(runDir)\n",
        "\n",
        "for i, td in enumerate(collector):\n",
        "    td = td.clone()\n",
        "    # Clip rewards for stability in discrete action spaces\n",
        "    if not cont:\n",
        "        td[\"next\", \"reward\"] = torch.clamp(td[\"next\", \"reward\"], -1.0, 1.0)\n",
        "    \n",
        "    # We may want to normalize the rewards in continuous action spaces, by dividing by 100\n",
        "\n",
        "\n",
        "    # Compute advantage + value targets\n",
        "    td = gae_module(td)\n",
        "\n",
        "    # Normalize Advantages\n",
        "    adv = td[\"advantage\"]\n",
        "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "    td.set(\"advantage\", adv)\n",
        "\n",
        "    # Update entropy coefficient\n",
        "    with torch.no_grad():\n",
        "        coef = get_entropy_coef(i, num_iterations)\n",
        "        ppo_loss.entropy_coef.copy_(torch.tensor(coef, device=ppo_loss.entropy_coef.device))\n",
        "\n",
        "    # Sample minibatches\n",
        "    replay_buffer.extend(td)\n",
        "\n",
        "    for _ in range(num_epochs):\n",
        "        sample_td = replay_buffer.sample()\n",
        "\n",
        "        # Compute loss\n",
        "        loss_vals = ppo_loss(sample_td)\n",
        "        loss_value = (\n",
        "            loss_vals[\"loss_objective\"]\n",
        "            + loss_vals[\"loss_critic\"]\n",
        "            + loss_vals[\"loss_entropy\"]\n",
        "        )\n",
        "\n",
        "        # Backpropagate and optimize\n",
        "        loss_value.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(ppo_loss.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    replay_buffer.empty()\n",
        "\n",
        "    logs[\"train_reward\"].append(td[\"next\", \"reward\"].mean().item())\n",
        "\n",
        "    if i % checkpoint_interval == 0:\n",
        "        # Save the evaluation data\n",
        "        with (set_exploration_type(ExplorationType.DETERMINISTIC)), torch.no_grad():\n",
        "          # Run the env with the actors value\n",
        "          eval_rollout = env.rollout(1024, actor)\n",
        "\n",
        "          # Save the evaluation data\n",
        "          logs[\"eval_reward\"].append( eval_rollout[\"next\", \"reward\"].mean().item() )\n",
        "          #logs[\"eval_steps\"].append( eval_rollout[\"step_count\"].min().item() )\n",
        "          del eval_rollout\n",
        "\n",
        "          # Save a checkpoint\n",
        "          filename = f'{runDir}/{i}.ch'\n",
        "          checkpoint = {\n",
        "              'model_state_dict': actor.module.state_dict(),\n",
        "          }\n",
        "\n",
        "          torch.save(checkpoint, filename)\n",
        "\n",
        "    pbar.set_description(\"Training \")\n",
        "\n",
        "    plot(logs)\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "    scheduler.step()\n",
        "pbar.close()\n",
        "\n",
        "# Final save of the model\n",
        "filename = f'{runDir}/_final.ch'\n",
        "checkpoint = {\n",
        "    'model_state_dict': actor.module.state_dict(),\n",
        "}\n",
        "torch.save(checkpoint, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0eb4a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(f'{runDir}/_logs.json', 'w') as json_file:\n",
        "    json.dump(logs, json_file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
