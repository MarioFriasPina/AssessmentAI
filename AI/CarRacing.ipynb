{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YOJvphymrXVe",
      "metadata": {
        "id": "YOJvphymrXVe"
      },
      "outputs": [],
      "source": [
        "#!apt-get update\n",
        "#!apt-get install -y swig python3-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b813e7",
      "metadata": {
        "id": "c0b813e7"
      },
      "outputs": [],
      "source": [
        "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36960300",
      "metadata": {
        "id": "36960300"
      },
      "source": [
        "## PPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc4177d",
      "metadata": {
        "id": "4dc4177d"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from torch import multiprocessing\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Information\n",
        "from IPython.display import clear_output, display\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import OneHotCategorical\n",
        "\n",
        "# TorchRL\n",
        "from torchrl.envs.transforms import (\n",
        "    TransformedEnv, Compose, ToTensorImage, ObservationNorm, StepCounter, DoubleToFloat, GrayScale, CatFrames, UnsqueezeTransform\n",
        "    )\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "from torchrl.modules import ProbabilisticActor, SafeModule, ValueOperator, TanhNormal\n",
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.objectives.value import GAE\n",
        "\n",
        "# Environment\n",
        "from torchrl.envs.libs.gym import GymEnv\n",
        "\n",
        "# Other\n",
        "import uuid\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7882c92",
      "metadata": {
        "id": "c7882c92"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd75e3ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd75e3ec",
        "outputId": "d04d01c7-b590-4c2b-f424-c0d3526bad5d"
      },
      "outputs": [],
      "source": [
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = (\n",
        "    torch.device(0)\n",
        "    if torch.cuda.is_available() and not is_fork\n",
        "    else torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Collector hyper-parameters\n",
        "frames_per_batch = 1024 # number of frames collected per batch\n",
        "num_iterations = 256 # number of batches\n",
        "total_frames = num_iterations * frames_per_batch  # total number of frames to collect\n",
        "\n",
        "# PPO hyper-parameters\n",
        "sub_batch_size = 128  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
        "num_epochs = 8 # optimization steps per batch of data collected\n",
        "learning_rate = 2e-4 # learning rate for the optimizer\n",
        "\n",
        "# Checkpoint saving parameters\n",
        "checkpoint_interval = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af235e9",
      "metadata": {
        "id": "5af235e9"
      },
      "source": [
        "### Creating the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b586b0b",
      "metadata": {
        "id": "6b586b0b"
      },
      "outputs": [],
      "source": [
        "cont = False  # whether to use continuous actions\n",
        "\n",
        "base_env = GymEnv(\"CarRacing-v3\", continuous=cont, render_mode=\"rgb_array\", device=device)\n",
        "\n",
        "# Compose them into a TransformedEnv\n",
        "env = TransformedEnv(base_env,\n",
        "    Compose(\n",
        "        DoubleToFloat(),\n",
        "        ToTensorImage(),\n",
        "        GrayScale(),\n",
        "        UnsqueezeTransform(-4),\n",
        "        CatFrames(dim=-3, N=4),\n",
        "        ObservationNorm(in_keys=[\"pixels\"]),\n",
        "        StepCounter()\n",
        "    )\n",
        ")\n",
        "\n",
        "# Normalize observations\n",
        "env.transform[-2].init_stats(num_iter=256, reduce_dim=0, cat_dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b01750",
      "metadata": {
        "id": "94b01750"
      },
      "source": [
        "### Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b51dd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b51dd4",
        "outputId": "5b960600-886e-4f6b-e863-5678cdf6b830"
      },
      "outputs": [],
      "source": [
        "class CarRacingCritic(nn.Module):\n",
        "    def __init__(self, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "        super().__init__()\n",
        "        # Input has 4 channels (stacked grayscale frames)\n",
        "        self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "        H, W = img_size\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, n_frames, H, W, device=device)\n",
        "            o = self.conv1(dummy)\n",
        "            o = self.conv2(o)\n",
        "            o = self.conv3(o)\n",
        "            self.flatten_size = o.view(1, -1).shape[1]\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "        self.value_head = nn.Linear(512, 1, device=device)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        if obs.dim() == 3:\n",
        "            obs = obs.unsqueeze(0)\n",
        "        x = torch.relu(self.conv1(obs))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        return self.value_head(x)\n",
        "\n",
        "if (cont):\n",
        "    \"\"\" For continuous actions: \"\"\"\n",
        "    class CarRacingContinuous(nn.Module):\n",
        "        def __init__(self, n_actions: int, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "            super().__init__()\n",
        "            # Input has 4 channels (stacked grayscale frames)\n",
        "            self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "            H, W = img_size\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, n_frames, H, W, device=device)\n",
        "                o = self.conv1(dummy)\n",
        "                o = self.conv2(o)\n",
        "                o = self.conv3(o)\n",
        "                self.flatten_size = o.view(1, -1).shape[1]\n",
        "            self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "\n",
        "            # For continuous actions: we output a 3-element mean vector (steer, gas, brake)\n",
        "            self.action_mean = nn.Linear(512, n_actions, device=device)\n",
        "            # and a log standard deviation\n",
        "            self.log_std = nn.Parameter(torch.zeros(n_actions, device=device))\n",
        "    \n",
        "        def forward(self, obs: torch.Tensor):\n",
        "            if obs.dim() == 3:\n",
        "                obs = obs.unsqueeze(0)\n",
        "            x = torch.relu(self.conv1(obs))\n",
        "            x = torch.relu(self.conv2(x))\n",
        "            x = torch.relu(self.conv3(x))\n",
        "            x = x.reshape(x.shape[0], -1)\n",
        "            x = torch.relu(self.fc1(x))\n",
        "            mean = self.action_mean(x)        # → [B, 3]\n",
        "            # expand log_std to [B, 3] automatically via broadcasting:\n",
        "            log_std = self.log_std.unsqueeze(0).expand_as(mean)\n",
        "            return mean, log_std              # → two outputs: mean and log_std\n",
        "\n",
        "    backbone_net = SafeModule(\n",
        "        module=CarRacingContinuous(env.action_spec.shape.numel()),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"action_mean\", \"action_log_std\"],\n",
        "    )\n",
        "\n",
        "    actor = ProbabilisticActor(\n",
        "        module=backbone_net,\n",
        "        spec=env.action_spec,\n",
        "        in_keys=[\"action_mean\", \"action_log_std\"],\n",
        "        distribution_class=TanhNormal,\n",
        "        out_keys=[\"action\"],\n",
        "        return_log_prob=True,\n",
        "    )\n",
        "\n",
        "    critic_net = SafeModule(\n",
        "        module=CarRacingCritic(),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "    critic = ValueOperator(\n",
        "        module=critic_net,\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "\n",
        "else:\n",
        "    \"\"\" For discrete actions: \"\"\"\n",
        "    class CarRacingDiscrete(nn.Module):\n",
        "        def __init__(self, n_actions: int, n_frames: int = 4, img_size: tuple = (96, 96)):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(n_frames, 32, kernel_size=8, stride=4, device=device)\n",
        "            self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device)\n",
        "            self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device)\n",
        "\n",
        "            # Dynamically compute flatten_size so we never hard‐code incorrectly:\n",
        "            H, W = img_size\n",
        "            with torch.no_grad():\n",
        "                dummy = torch.zeros(1, n_frames, H, W, device=device)  # [1, 3, 96, 96]\n",
        "                o = self.conv1(dummy)\n",
        "                o = self.conv2(o)\n",
        "                o = self.conv3(o)\n",
        "                self.flatten_size = o.view(1, -1).shape[1]  # e.g. 4096\n",
        "\n",
        "            self.fc1 = nn.Linear(self.flatten_size, 512, device=device)\n",
        "            self.logits = nn.Linear(512, n_actions, device=device)\n",
        "\n",
        "        def forward(self, obs: torch.Tensor):\n",
        "            # If obs has shape [3, 96, 96], unsqueeze so it becomes [1, 3, 96, 96]:\n",
        "            if obs.dim() == 3:                     # no batch dimension\n",
        "                obs = obs.unsqueeze(0)             # now [1, 3, 96, 96]\n",
        "\n",
        "            # By this point, obs must be [B, 3, 96, 96]:\n",
        "            x = torch.relu(self.conv1(obs))        # → [B, 32, 23, 23]\n",
        "            x = torch.relu(self.conv2(x))          # → [B, 64, 10, 10]\n",
        "            x = torch.relu(self.conv3(x))          # → [B, 64,  8,  8]\n",
        "            x = x.reshape(x.shape[0], -1)             # → [B, 4096]  (because flatten_size = 4096)\n",
        "            x = torch.relu(self.fc1(x))            # → [B, 512]\n",
        "            return self.logits(x)                  # → [B, n_actions]\n",
        "\n",
        "    backbone_net = SafeModule(\n",
        "        module=CarRacingDiscrete(env.action_spec.shape.numel()),\n",
        "        in_keys=[\"pixels\"],     # expects obs under key \"pixels\"\n",
        "        out_keys=[\"logits\"],    # produces a \"logits\" tensor\n",
        "    )\n",
        "\n",
        "    actor = ProbabilisticActor(\n",
        "        module=backbone_net,\n",
        "        spec=env.action_spec,               # DiscreteTensorSpec\n",
        "        in_keys=[\"logits\"],                 # read logits from that key\n",
        "        distribution_class=OneHotCategorical,     # TorchRL’s one-hot categorical: samples a one-hot vector of size n_actions\n",
        "        out_keys=[\"action\"],                # writes a one-hot action into \"action\"\n",
        "        return_log_prob=True,               # store \"log_prob\" in the tensordict\n",
        "    )\n",
        "\n",
        "    critic_net = SafeModule(\n",
        "        module=CarRacingCritic(),\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )\n",
        "    critic = ValueOperator(\n",
        "        module=critic_net,\n",
        "        in_keys=[\"pixels\"],\n",
        "        out_keys=[\"state_value\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a59bd47",
      "metadata": {
        "id": "9a59bd47"
      },
      "source": [
        "### Data Collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f37e84c1",
      "metadata": {
        "id": "f37e84c1"
      },
      "outputs": [],
      "source": [
        "replay_buffer = ReplayBuffer(\n",
        "    storage=LazyTensorStorage(max_size=frames_per_batch, device=device),\n",
        "    sampler=SamplerWithoutReplacement(),\n",
        "    batch_size=sub_batch_size,\n",
        ")\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    policy=actor,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        "    device=device,\n",
        "    split_trajs=False,\n",
        ")\n",
        "\n",
        "gae_module = GAE(\n",
        "    value_network=critic,\n",
        "    gamma=0.99,\n",
        "    lmbda=0.95,\n",
        ")\n",
        "\n",
        "def get_entropy_coef(iteration, total_iters):\n",
        "    if iteration < 0.3 * total_iters:\n",
        "        return 0.03\n",
        "    elif iteration < 0.6 * total_iters:\n",
        "        return 0.015\n",
        "    else:\n",
        "        return 0.005\n",
        "\n",
        "ppo_loss = ClipPPOLoss(\n",
        "    actor_network=actor,\n",
        "    critic_network=critic,\n",
        "    clip_epsilon=0.2,\n",
        "    loss_critic_type=\"smooth_l1\",\n",
        "    \n",
        "    #normalize_advantage=True,\n",
        "\n",
        "    entropy_coef=get_entropy_coef(0, num_iterations),\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(ppo_loss.parameters(), lr=2e-4)\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ChainedScheduler([\n",
        "    torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-2, total_iters=int(0.1 * num_iterations)), # First 10% of the training\n",
        "    torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(0.9 * num_iterations), eta_min=1e-8), # Last 90% of the training\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "375cea7d",
      "metadata": {
        "id": "375cea7d"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cZeNveQfv-Tk",
      "metadata": {
        "id": "cZeNveQfv-Tk"
      },
      "outputs": [],
      "source": [
        "def plot(logs):\n",
        "    # Update plot data\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Rebuild the figure from scratch\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    axes = axes.flatten()\n",
        "    titles = [\n",
        "        \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
        "        #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
        "    ]\n",
        "    data = [\n",
        "        (logs[\"train_reward\"],   \"blue\"),\n",
        "        (logs[\"eval_reward\"],    \"green\"),\n",
        "        #(logs[\"train_steps_min\"],    \"red\"),\n",
        "        #(logs[\"train_steps_max\"],     \"orange\"),\n",
        "    ]\n",
        "\n",
        "    for ax, title, (y, color) in zip(axes, titles, data):\n",
        "        ax.plot(y, color=color)\n",
        "        ax.set_title(title)\n",
        "        ax.relim()\n",
        "        ax.autoscale_view()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display the new figure\n",
        "    display(fig)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f654737",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "5f654737",
        "outputId": "4f1d13c9-65f1-4525-90dc-b8256cc203a6"
      },
      "outputs": [],
      "source": [
        "pbar = tqdm(total=num_iterations)\n",
        "pbar.set_description(\"Training \")\n",
        "logs = defaultdict(list)\n",
        "\n",
        "runDir = f'./checkpoints/{uuid.uuid4()}'\n",
        "os.mkdir(runDir)\n",
        "\n",
        "for i, td in enumerate(collector):\n",
        "    td = td.clone()\n",
        "    # Clip rewards for stability in discrete action spaces\n",
        "    if not cont:\n",
        "        td[\"next\", \"reward\"] = torch.clamp(td[\"next\", \"reward\"], -1.0, 1.0)\n",
        "    \n",
        "    # We may want to normalize the rewards in continuous action spaces, by dividing by 100\n",
        "\n",
        "\n",
        "    # Compute advantage + value targets\n",
        "    td = gae_module(td)\n",
        "\n",
        "    # Normalize Advantages\n",
        "    adv = td[\"advantage\"]\n",
        "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "    td.set(\"advantage\", adv)\n",
        "\n",
        "    # Update entropy coefficient\n",
        "    with torch.no_grad():\n",
        "        coef = get_entropy_coef(i, num_iterations)\n",
        "        ppo_loss.entropy_coef.copy_(torch.tensor(coef, device=ppo_loss.entropy_coef.device))\n",
        "\n",
        "    # Sample minibatches\n",
        "    replay_buffer.extend(td)\n",
        "\n",
        "    for _ in range(num_epochs):\n",
        "        sample_td = replay_buffer.sample()\n",
        "\n",
        "        # Compute loss\n",
        "        loss_vals = ppo_loss(sample_td)\n",
        "        loss_value = (\n",
        "            loss_vals[\"loss_objective\"]\n",
        "            + loss_vals[\"loss_critic\"]\n",
        "            + loss_vals[\"loss_entropy\"]\n",
        "        )\n",
        "\n",
        "        # Backpropagate and optimize\n",
        "        loss_value.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(ppo_loss.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    replay_buffer.empty()\n",
        "\n",
        "    logs[\"train_reward\"].append(td[\"next\", \"reward\"].mean().item())\n",
        "\n",
        "    if i % checkpoint_interval == 0:\n",
        "        # Save the evaluation data\n",
        "        with (set_exploration_type(ExplorationType.DETERMINISTIC)), torch.no_grad():\n",
        "          # Run the env with the actors value\n",
        "          eval_rollout = env.rollout(1024, actor)\n",
        "\n",
        "          # Save the evaluation data\n",
        "          logs[\"eval_reward\"].append( eval_rollout[\"next\", \"reward\"].mean().item() )\n",
        "          #logs[\"eval_steps\"].append( eval_rollout[\"step_count\"].min().item() )\n",
        "          del eval_rollout\n",
        "\n",
        "          # Save a checkpoint\n",
        "          filename = f'{runDir}/{i}.ch'\n",
        "          checkpoint = {\n",
        "              'model_state_dict': actor.module.state_dict(),\n",
        "          }\n",
        "\n",
        "          torch.save(checkpoint, filename)\n",
        "\n",
        "    pbar.set_description(\"Training \")\n",
        "\n",
        "    plot(logs)\n",
        "\n",
        "    pbar.update(1)\n",
        "\n",
        "    scheduler.step()\n",
        "pbar.close()\n",
        "\n",
        "# Final save of the model\n",
        "filename = f'{runDir}/_final.ch'\n",
        "checkpoint = {\n",
        "    'model_state_dict': actor.module.state_dict(),\n",
        "}\n",
        "torch.save(checkpoint, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0eb4a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(f'{runDir}/_logs.json', 'w') as json_file:\n",
        "    json.dump(logs, json_file, indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
