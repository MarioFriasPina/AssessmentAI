{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y swig python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # PPO en CarRacing-v3 con TorchRL (con corrección en make_torchrl_env)\n",
    "#\n",
    "# Este Notebook muestra cómo entrenar CarRacing-v3 con PPO usando únicamente TorchRL (sin escribir PPO a mano),\n",
    "# e incluye gráficos en vivo y una barra de progreso (`tqdm`) que muestra “Avg Reward (Train)” y “Avg Reward (Eval)”.\n",
    "#\n",
    "# **Dependencias**:\n",
    "# ```\n",
    "# pip install torch torchvision torchrl gymnasium[box2d] tqdm matplotlib\n",
    "# ```\n",
    "#\n",
    "# ———————————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Importaciones y utilidades de graficado\n",
    "#\n",
    "# Importamos Torch, TorchRL, Gym, Matplotlib, `tqdm`, y definimos la función `plot(logs)` para reconstruir la figura en cada iteración.\n",
    "\n",
    "# %% [code]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import ToTensorImage, Compose\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import AutoResetEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "from torchrl.modules import SafeModule, ProbabilisticActor, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# — Función de graficado (idéntica a la del IPYNB de ejemplo) —\n",
    "def plot(logs):\n",
    "    \"\"\"\n",
    "    Recibe un dict `logs` con listas en:\n",
    "      logs[\"train_reward\"] = [r1, r2, r3, ...]\n",
    "      logs[\"eval_reward\"]  = [e1, e2, e3, ...]\n",
    "    y dibuja dos subplots: \n",
    "      · Avg Reward (Train)\n",
    "      · Avg Reward (Eval)\n",
    "    Llamar esta función tras actualizar `logs` para redibujar la gráfica en línea.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\"Avg Reward (Train)\", \"Avg Reward (Eval)\"]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"], \"blue\"),\n",
    "        (logs[\"eval_reward\"],  \"green\"),\n",
    "    ]\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Wrapper discreto para CarRacing-v3\n",
    "#\n",
    "# El entorno original de CarRacing usa un espacio continuo de 3 dimensiones (`[steer, gas, brake]`),\n",
    "# pero queremos usar PPO discreto con 5 acciones:\n",
    "#\n",
    "# 1. `0`: no-op            → `[ 0.0,  0.0,  0.0 ]`  \n",
    "# 2. `1`: acelerar        → `[ 0.0,  1.0,  0.0 ]`  \n",
    "# 3. `2`: frenar          → `[ 0.0,  0.0,  0.8 ]`  \n",
    "# 4. `3`: girar izquierda → `[−1.0,  0.0,  0.0 ]`  \n",
    "# 5. `4`: girar derecha   → `[ 1.0,  0.0,  0.0 ]`\n",
    "#\n",
    "# Este wrapper convierte un entero discreto (0–4) en el vector contínuo que CarRacing espera.\n",
    "\n",
    "# %% [code]\n",
    "class DiscreteCarRacingWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Convierte la acción discreta en un vector [steer, gas, brake].\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([ 0.0,  0.0,  0.0 ], dtype=np.float32),  # 0: no-op\n",
    "            np.array([ 0.0,  1.0,  0.0 ], dtype=np.float32),  # 1: acelerar\n",
    "            np.array([ 0.0,  0.0,  0.8 ], dtype=np.float32),  # 2: frenar\n",
    "            np.array([-1.0,  0.0,  0.0 ], dtype=np.float32),  # 3: girar izquierda\n",
    "            np.array([ 1.0,  0.0,  0.0 ], dtype=np.float32),  # 4: girar derecha\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "\n",
    "    def action(self, action_discrete: int) -> np.ndarray:\n",
    "        return self.discrete_actions[int(action_discrete)]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Corrección en make_torchrl_env\n",
    "#\n",
    "# Ahora usamos `env_ctor` para que `GymEnv` construya internamente el entorno ya envuelto,\n",
    "# y semillamos **antes** de envolver, evitando el error de `seed`.\n",
    "\n",
    "# %% [code]\n",
    "def make_torchrl_env(seed: int = 0, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Crea un GymEnv de TorchRL para CarRacing-v3 con acciones discretas.\n",
    "    Usamos `env_ctor=` para que GymEnv construya internamente el entorno ya envuelto.\n",
    "    \"\"\"\n",
    "    # 1) Defino la función que al llamarla crea y semilla el entorno base\n",
    "    def _make_wrapped_env():\n",
    "        base_env = gym.make(\"CarRacing-v3\", render_mode=None)\n",
    "        # Semilleo directamente el entorno base\n",
    "        base_env.reset(seed=seed)\n",
    "        # Aplico el wrapper discreto\n",
    "        wrapped = DiscreteCarRacingWrapper(base_env)\n",
    "        return wrapped\n",
    "\n",
    "    # 2) Construyo el GymEnv usando `env_ctor` (la función que hace _make_wrapped_env)\n",
    "    trl_env = GymEnv(\n",
    "        env_ctor=_make_wrapped_env,\n",
    "        from_pixels=True,\n",
    "        pixels_only=True,\n",
    "        device=device\n",
    "    )\n",
    "    # 3) Aplico la transformación de píxeles: uint8 H×W×C → float32 C×H×W en [0,1]\n",
    "    trl_env.set_transform(Compose(ToTensorImage()))\n",
    "    return trl_env\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Definir Actor–Crítico con SafeModule + ProbabilisticActor + ValueOperator\n",
    "#\n",
    "# - La red convolucional: 3 → 32 conv8×8/stride 4 → 64 conv4×4/stride 2 → 64 conv3×3/stride 1 → Flatten → FC 512.  \n",
    "# - **Policy head**: saca logits sobre 5 acciones discretas.  \n",
    "# - **Value head**: saca un escalar (valor del estado).  \n",
    "\n",
    "# %% [code]\n",
    "# 4.1) Red CNN base\n",
    "class CNNBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)   # → [32,23,23]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  # → [64,10,10]\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  # → [64,8,8]\n",
    "        self.flatten_dim = 64 * 8 * 8\n",
    "        self.fc = nn.Linear(self.flatten_dim, 512)\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(obs_image))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.flatten_dim)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x  # → [B,512]\n",
    "\n",
    "# 4.2) Construcción del módulo completo con dos salidas\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, num_actions: int = 5):\n",
    "        super().__init__()\n",
    "        self.body = CNNBody()\n",
    "        # Cabeza de política: produce logits [B,5]\n",
    "        self.policy_head = nn.Linear(512, num_actions)\n",
    "        # Cabeza de valor: produce valor escalar [B,1]\n",
    "        self.value_head  = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor):\n",
    "        features = self.body(obs_image)                    # [B,512]\n",
    "        logits   = self.policy_head(features)              # [B,5]\n",
    "        value    = self.value_head(features).squeeze(-1)   # [B]\n",
    "        return logits, value\n",
    "\n",
    "# 4.3) Armado de SafeModule + ProbabilisticActor + ValueOperator\n",
    "def make_ac_modules(device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - actor_module: ProbabilisticActor que espera 'pixels' y saca 'action', 'log_prob'\n",
    "      - value_module: ValueOperator que espera 'pixels' y saca 'state_value'\n",
    "    \"\"\"\n",
    "    base_net = ActorCriticNet(num_actions=5).to(device)\n",
    "    # Política\n",
    "    actor_sm = SafeModule(\n",
    "        module=base_net,\n",
    "        in_keys=[\"pixels\"],            # recibe tensor 'pixels'\n",
    "        out_keys=[\"logits\", \"value\"],  # forward devuelve (logits, value)\n",
    "        function=lambda m, x: m(x)     # m(x) → (logits,value)\n",
    "    )\n",
    "    actor_module = ProbabilisticActor(\n",
    "        module=actor_sm,\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\", \"log_prob\"],\n",
    "        distribution_class=Categorical,\n",
    "        distribution_kwargs={},       # Categorical(logits=logits)\n",
    "        return_log_prob=True,\n",
    "        default_interaction_mode=\"random\",  # en train samplea, en eval hará mode=\"mode\"\n",
    "    )\n",
    "\n",
    "    # Valor\n",
    "    # SafeModule que toma (logits, value) y extrae value para ValueOperator\n",
    "    value_sm = SafeModule(\n",
    "        module=base_net,\n",
    "        in_keys=[\"pixels\"],\n",
    "        out_keys=[\"state_value\"], \n",
    "        function=lambda m, x: m(x)[1].unsqueeze(-1)  \n",
    "        # m(x)[1] es [B], lo llevamos a [B,1] para ValueOperator\n",
    "    )\n",
    "    value_module = ValueOperator(\n",
    "        in_keys=[\"state_value\"],   # recibe 'state_value'\n",
    "        out_keys=[\"state_value\"],  # devuelve también 'state_value'\n",
    "    )\n",
    "\n",
    "    return actor_module, value_module\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Configuración del Collector y pérdida PPO (ClipPPOLoss + GAE)\n",
    "#\n",
    "# - `frames_per_batch`: cuántos pasos (frames) recolectamos antes de hacer update de PPO.  \n",
    "# - `num_iterations`: cuántas iteraciones (épocas) de PPO.  \n",
    "# - Internamente, `ClipPPOLoss` calculará la pérdida de crítico (MSE), la pérdida de política con clipping, y entropía.  \n",
    "# - Para GAE definimos un objeto `GAE(gamma, lam)`.\n",
    "\n",
    "# %% [code]\n",
    "# 5.1) Hiperparámetros\n",
    "frames_per_batch   = 1024    # pasos a recolectar por iteración\n",
    "num_iterations     = 256     # cuántas iteraciones (épocas) de PPO\n",
    "ppo_epoch          = 4       # cuántas pasadas sobre cada batch de datos\n",
    "sub_batch_size     = 128     # minibatch para optimizador\n",
    "gamma              = 0.99\n",
    "lam                = 0.95\n",
    "learning_rate      = 2e-4\n",
    "\n",
    "# 5.2) Crear entornos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env_train = make_torchrl_env(seed=123, device=device)\n",
    "env_eval  = make_torchrl_env(seed=456, device=device)\n",
    "\n",
    "# 5.3) Crear módulos Actor y Crítico\n",
    "actor_module, value_module = make_ac_modules(device=device)\n",
    "\n",
    "# 5.4) Establecer exploración por defecto (importante para PPO “on-policy”)\n",
    "set_exploration_type(ExplorationType.RANDOM)\n",
    "\n",
    "# 5.5) ClipPPOLoss y GAE\n",
    "clip_ppo_loss = ClipPPOLoss(\n",
    "    actor=actor_module,\n",
    "    value_network=value_module,\n",
    "    clip_eps=0.2,\n",
    "    value_loss_coeff=0.5,\n",
    "    entropy_coeff=0.01,\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "\n",
    "# 5.6) Optimizador para actor + crítico\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(actor_module.parameters()) + list(value_module.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# 5.7) GAE para calcular ventajas\n",
    "gae_module = GAE(\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "# 5.8) Collector síncrono con AutoResetEnv para recolección de transiciones\n",
    "train_collector = SyncDataCollector(\n",
    "    env=AutoResetEnv(env_train),\n",
    "    policy=actor_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=None,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Rutina de entrenamiento + evaluación con tqdm y plot(logs)\n",
    "#\n",
    "# En cada iteración (de `0` a `num_iterations-1`):\n",
    "# 1. Recolectar `frames_per_batch` pasos con `train_collector`.  \n",
    "# 2. Calcular ventajas y retornos con `gae_module`.  \n",
    "# 3. Llamar a `clip_ppo_loss` para optimizar política + crítico.  \n",
    "# 4. Calcular `avg_train_reward` de ese batch.  \n",
    "# 5. Ejecutar `num_eval_episodes` episodios en `env_eval` para `avg_eval_reward`.  \n",
    "# 6. Actualizar la barra de progreso (`tqdm`) y guardar métricas en `logs`.  \n",
    "# 7. Llamar a `plot(logs)` para redibujar en línea.\n",
    "\n",
    "# %% [code]\n",
    "# 6.1) Variables para logs\n",
    "logs = {\n",
    "    \"train_reward\": [],\n",
    "    \"eval_reward\":  []\n",
    "}\n",
    "\n",
    "num_eval_episodes = 3  # cuántos episodios de evaluación por iteración\n",
    "\n",
    "# Barra de progreso\n",
    "pbar = tqdm(range(num_iterations), desc=\"PPO Entrenando\", unit=\"iter\")\n",
    "\n",
    "for iter_idx in pbar:\n",
    "    # —————————————————————————————————————————————————\n",
    "    # 1) RECOLECTAR frames_per_batch pasos (rollout on-policy)\n",
    "    # —————————————————————————————————————————————————\n",
    "    experience = next(train_collector)\n",
    "    # experience es un TensorDict con keys:\n",
    "    #   \"pixels\", \"action\", \"log_prob\", \"state_value\", \"next_pixels\", \"reward\", \"done\", \"rollout_info\", ...\n",
    "\n",
    "    # 2) CALCULAR VENTAJAS Y RETORNOS (GAE)\n",
    "    # GAE rellena \"advantage\" y \"return\" en el mismo TensorDict.\n",
    "    _ = gae_module(experience)\n",
    "\n",
    "    # 3) OPTIMIZAR CON ClipPPOLoss\n",
    "    # Le pasamos `experience`, `optimizer`, `ppo_epoch`, `sub_batch_size`\n",
    "    loss_ppo = clip_ppo_loss(\n",
    "        experience,\n",
    "        optimizer=optimizer,\n",
    "        ppo_epoch=ppo_epoch,\n",
    "        mini_batch_size=sub_batch_size,\n",
    "    )\n",
    "\n",
    "    # 4) CALCULAR avg_train_reward para este batch \n",
    "    # TorchRL almacena en \"rollout_info\" → \"reward_sum\" la suma de rewards por episodio.\n",
    "    train_episode_rewards = experience.get((\"rollout_info\", \"reward_sum\")).cpu().numpy()\n",
    "    avg_train_reward = float(np.mean(train_episode_rewards))\n",
    "    logs[\"train_reward\"].append(avg_train_reward)\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # 5) EVALUACIÓN (política determinista)\n",
    "    # —————————————————————————————————————————————————\n",
    "    set_exploration_type(ExplorationType.MODE)  # fuerza argmax en ProbabilisticActor\n",
    "    eval_episode_rewards = []\n",
    "    for _ in range(num_eval_episodes):\n",
    "        td = env_eval.reset()  # td contiene \"pixels\"\n",
    "        done = False\n",
    "        ep_rew = 0.0\n",
    "        while not done:\n",
    "            obs_td = td.clone().to(device)\n",
    "            out = actor_module(obs_td)\n",
    "            action = out.get(\"action\").item()\n",
    "            td, reward, term, trunc, _ = env_eval.step(action)\n",
    "            done = bool(term or trunc)\n",
    "            ep_rew += reward\n",
    "        eval_episode_rewards.append(ep_rew)\n",
    "    avg_eval_reward = float(np.mean(eval_episode_rewards))\n",
    "    logs[\"eval_reward\"].append(avg_eval_reward)\n",
    "    set_exploration_type(ExplorationType.RANDOM)\n",
    "\n",
    "    # 6) ACTUALIZAR tqdm Y GRAficar\n",
    "    pbar.set_postfix({\n",
    "        \"avg_train\": f\"{avg_train_reward:.1f}\",\n",
    "        \"avg_eval\":  f\"{avg_eval_reward:.1f}\"\n",
    "    })\n",
    "    plot(logs)\n",
    "\n",
    "# Cerrar barra\n",
    "pbar.close()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Guardar modelo final\n",
    "#\n",
    "# Si quieres salvar el checkpoint del actor y el crítico, puedes hacer:\n",
    "# ```python\n",
    "# save_name = f\"ppo_cr_{uuid.uuid4().hex[:6]}.pt\"\n",
    "# checkpoint = {\n",
    "#     \"actor_state_dict\": actor_module.state_dict(),\n",
    "#     \"value_state_dict\": value_module.state_dict(),\n",
    "# }\n",
    "# torch.save(checkpoint, save_name)\n",
    "# print(\"Modelo guardado en\", save_name)\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Observaciones y consejos finales\n",
    "#\n",
    "# 1. **Hyperparámetros**  \n",
    "#    - `frames_per_batch` ≈ 1024–4096 (depende de tu GPU/CPU).  \n",
    "#    - `num_iterations` = 200–300 (si quieres entrenar más tiempo, subirlo).  \n",
    "#    - `ppo_epoch` = 3–6, `sub_batch_size` = 128.  \n",
    "#    - `gamma` = 0.99, `lam` = 0.95, `clip_eps` = 0.2.  \n",
    "#    - `learning_rate` = 2 × 10⁻⁴.  \n",
    "#\n",
    "# 2. **Funciones de TorchRL**  \n",
    "#    - `ClipPPOLoss` engloba:  \n",
    "#       • Cálculo del ratio πθ/πθ₀ con clipping.  \n",
    "#       • Loss de valor (MSE) con coeficiente `value_loss_coeff`.  \n",
    "#       • Término de entropía (`entropy_coeff`).  \n",
    "#    - `GAE` crea las llaves `\"advantage\"` y `\"return\"` dentro del TensorDict.  \n",
    "#    - `SyncDataCollector` con `frames_per_batch` devuelve un TensorDict con:\n",
    "#       • `\"rollout_info\",\"reward_sum\"`: suma de recompensas por episodio.  \n",
    "#\n",
    "# 3. **Gráficas**  \n",
    "#    - Cada iteración `plot(logs)` reconstruye la figura desde cero (con `clear_output(wait=True)` y `display(fig)`),\n",
    "#      de modo que verás en vivo cómo se actualizan las curvas de “Avg Reward (Train)” y “Avg Reward (Eval)”.  \n",
    "#\n",
    "# 4. **Exploración**  \n",
    "#    - Usamos `set_exploration_type(ExplorationType.RANDOM)` durante el entrenamiento para que `ProbabilisticActor` samplee acciones.  \n",
    "#    - Para evaluación, cambiamos a `ExplorationType.MODE` para usar la acción `argmax`.  \n",
    "#\n",
    "# 5. **Ajustes**  \n",
    "#    - Si deseas ver el entorno “real” en evaluación, podrías crear un Gym env aparte con `render_mode=\"human\"` y dibujar algunos episodios.  \n",
    "#    - Para guardarlo periódicamente, inserta un bloque dentro del bucle:  \n",
    "#      ```python\n",
    "#      if iter_idx % 10 == 0:\n",
    "#          torch.save(...).\n",
    "#      ```  \n",
    "#\n",
    "# ———————————————————————————————————————————————————————————————————————————————\n",
    "#\n",
    "# ¡Listo! Ahora tienes el código completo con la corrección en `make_torchrl_env` para evitar el error de `seed` y `env_name`.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
