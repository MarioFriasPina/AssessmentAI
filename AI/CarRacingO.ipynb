{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y swig python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # PPO en CarRacing-v3 (discreto) con TorchRL  \n",
    "#\n",
    "# Este notebook entrena CarRacing-v3 usando PPO (discreto) de TorchRL, y replica el flujo de tu notebook original para crear el entorno.\n",
    "# Incluye:\n",
    "# 1. Registro de un nuevo Gym env “DiscreteCarRacing-v3” que envuelve a CarRacing-v3 con acciones discretas.  \n",
    "# 2. Creación del entorno TorchRL con `GymEnv(\"DiscreteCarRacing-v3\", continuous=False, render_mode=\"rgb_array\", device=device)` + `TransformedEnv`/`Compose` tal como en tu notebook.  \n",
    "# 3. Definición del actor–crítico con `SafeModule`, `ProbabilisticActor` y `ValueOperator`.  \n",
    "# 4. Uso de `SyncDataCollector`, `ClipPPOLoss` y `GAE` para PPO “out-of-the-box” de TorchRL.  \n",
    "# 5. Barra de progreso (`tqdm`) y función `plot(logs)` idéntica a la tuya, para actualizar en vivo las curvas de “Avg Reward (Train)” y “Avg Reward (Eval)”.  \n",
    "#\n",
    "# **Dependencias**:\n",
    "# ```bash\n",
    "# pip install torch torchvision torchrl gymnasium[box2d] tqdm matplotlib\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Importaciones y utilidades de graficado\n",
    "\n",
    "# %% [code]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import (\n",
    "    DoubleToFloat, ToTensorImage, GrayScale, UnsqueezeTransform, CatFrames,\n",
    "    ObservationNorm, StepCounter, Compose\n",
    ")\n",
    "from torchrl.envs import TransformedEnv\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import AutoResetEnv\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "from torchrl.modules import SafeModule, ProbabilisticActor, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "\n",
    "def plot(logs):\n",
    "    \"\"\"\n",
    "    Recibe un dict `logs` con listas:\n",
    "      logs[\"train_reward\"] = [...]\n",
    "      logs[\"eval_reward\"]  = [...]\n",
    "    y dibuja dos subplots:\n",
    "      · Avg Reward (Train)\n",
    "      · Avg Reward (Eval)\n",
    "    Similar a la función de tu notebook original.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\"Avg Reward (Train)\", \"Avg Reward (Eval)\"]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"], \"blue\"),\n",
    "        (logs[\"eval_reward\"],  \"green\"),\n",
    "    ]\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Wrapper discreto para CarRacing-v3\n",
    "\n",
    "# %% [code]\n",
    "class DiscreteCarRacingWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Convierte cada acción discreta (0..4) en el vector continuo [steer, gas, brake].\n",
    "    0: no-op            → [ 0.0,  0.0,  0.0 ]\n",
    "    1: acelerar        → [ 0.0,  1.0,  0.0 ]\n",
    "    2: frenar          → [ 0.0,  0.0,  0.8 ]\n",
    "    3: girar izquierda → [−1.0,  0.0,  0.0 ]\n",
    "    4: girar derecha   → [ 1.0,  0.0,  0.0 ]\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([ 0.0,  0.0,  0.0 ], dtype=np.float32),\n",
    "            np.array([ 0.0,  1.0,  0.0 ], dtype=np.float32),\n",
    "            np.array([ 0.0,  0.0,  0.8 ], dtype=np.float32),\n",
    "            np.array([-1.0,  0.0,  0.0 ], dtype=np.float32),\n",
    "            np.array([ 1.0,  0.0,  0.0 ], dtype=np.float32),\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "\n",
    "    def action(self, action_discrete: int) -> np.ndarray:\n",
    "        return self.discrete_actions[int(action_discrete)]\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Registrar “DiscreteCarRacing-v3” en Gym\n",
    "#\n",
    "# Aquí creamos un nuevo id de entorno para Gym que internamente usa nuestro wrapper discreto.\n",
    "# De esta forma podemos usar `GymEnv(\"DiscreteCarRacing-v3\", ...)` sin errores de env_name.\n",
    "\n",
    "# %% [code]\n",
    "# Ignoramos excepción si ya está registrado\n",
    "try:\n",
    "    register(\n",
    "        id=\"DiscreteCarRacing-v3\",\n",
    "        entry_point=lambda **kwargs: DiscreteCarRacingWrapper(\n",
    "            gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "        )\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Crear entornos TorchRL (base “continuo” corregido a “discreto”)\n",
    "#\n",
    "# En tu notebook original usabas algo como:\n",
    "# ```\n",
    "# base_env = GymEnv(\"CarRacing-v3\", continuous=cont, render_mode=\"rgb_array\", device=device)\n",
    "# env = TransformedEnv(base_env, Compose(DoubleToFloat(), ToTensorImage(), GrayScale(), ...))\n",
    "# ```\n",
    "# Ahora reemplazamos `\"CarRacing-v3\"` con `\"DiscreteCarRacing-v3\"` y `continuous=False`.\n",
    "\n",
    "# %% [code]\n",
    "def make_torchrl_env(device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Crea un TransformedEnv para “DiscreteCarRacing-v3” siguiendo el patrón de tu notebook:\n",
    "    1. GymEnv(\"DiscreteCarRacing-v3\", continuous=False, render_mode=\"rgb_array\", device=device)\n",
    "    2. Transformaciones idénticas: DoubleToFloat, ToTensorImage, GrayScale, UnsqueezeTransform, CatFrames, ObservationNorm, StepCounter.\n",
    "    \"\"\"\n",
    "    # 1) GymEnv con nombre de entorno discreto\n",
    "    base_env = GymEnv(\n",
    "        \"DiscreteCarRacing-v3\",\n",
    "        continuous=False,\n",
    "        render_mode=\"rgb_array\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 2) Armar TransformedEnv con tus mismas transformaciones (4 frames apilados)\n",
    "    env = TransformedEnv(\n",
    "        base_env,\n",
    "        Compose(\n",
    "            DoubleToFloat(),            # uint8 → float32\n",
    "            ToTensorImage(),            # HWC → CHW en [0,1]\n",
    "            GrayScale(),                # pasar a escala de grises\n",
    "            UnsqueezeTransform(-4),     # agrega dimensión de batch “enmedio”\n",
    "            CatFrames(dim=-3, N=4),     # apila 4 frames para el agente recurrente\n",
    "            ObservationNorm(in_keys=[\"pixels\"]),  # normaliza píxeles\n",
    "            StepCounter()               # agrega key “step_count” si quieres\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Inicializar estadísticas de ObservationNorm (igual que en tu notebook)\n",
    "    env.transform[-2].init_stats(num_iter=256, reduce_dim=0, cat_dim=0)\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Definir Actor–Crítico con SafeModule + ProbabilisticActor + ValueOperator\n",
    "\n",
    "# %% [code]\n",
    "# 5.1) Cuerpo CNN (idéntico al tuyo, pero ajustado a input de 4 frames en escala de grises)\n",
    "class CNNBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Ahora la entrada es (4, 96, 96) en escala de grises, no 3 canales\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)   # → [32,23,23]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  # → [64,10,10]\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  # → [64,8,8]\n",
    "        self.flatten_dim = 64 * 8 * 8\n",
    "        self.fc = nn.Linear(self.flatten_dim, 512)\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(obs_image))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.flatten_dim)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x  # → [B,512]\n",
    "\n",
    "# 5.2) Red Actor–Crítico completa\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, num_actions: int = 5):\n",
    "        super().__init__()\n",
    "        self.body = CNNBody()\n",
    "        self.policy_head = nn.Linear(512, num_actions)  # logits [B,5]\n",
    "        self.value_head  = nn.Linear(512, 1)            # value [B,1]\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor):\n",
    "        features = self.body(obs_image)                  # [B,512]\n",
    "        logits   = self.policy_head(features)            # [B,5]\n",
    "        value    = self.value_head(features).squeeze(-1) # [B]\n",
    "        return logits, value\n",
    "\n",
    "# 5.3) SafeModule + ProbabilisticActor + ValueOperator\n",
    "def make_ac_modules(device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - actor_module: ProbabilisticActor que recibe 'pixels' y sale 'action', 'log_prob'\n",
    "      - value_module: ValueOperator que recibe 'pixels' y sale 'state_value'\n",
    "    \"\"\"\n",
    "    base_net = ActorCriticNet(num_actions=5).to(device)\n",
    "\n",
    "    # (a) Política → SafeModule produce (logits, value), luego ProbabilisticActor\n",
    "    actor_sm = SafeModule(\n",
    "        module=base_net,\n",
    "        in_keys=[\"pixels\"],\n",
    "        out_keys=[\"logits\", \"value\"],\n",
    "        function=lambda m, x: m(x)\n",
    "    )\n",
    "    actor_module = ProbabilisticActor(\n",
    "        module=actor_sm,\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\", \"log_prob\"],\n",
    "        distribution_class=Categorical,\n",
    "        distribution_kwargs={},         # Categorical(logits=logits)\n",
    "        return_log_prob=True,\n",
    "        default_interaction_mode=\"random\",  # samplea en entrenamiento\n",
    "    )\n",
    "\n",
    "    # (b) Crítico → SafeModule extrae “value” para ValueOperator\n",
    "    value_sm = SafeModule(\n",
    "        module=base_net,\n",
    "        in_keys=[\"pixels\"],\n",
    "        out_keys=[\"state_value\"],\n",
    "        function=lambda m, x: m(x)[1].unsqueeze(-1)\n",
    "    )\n",
    "    value_module = ValueOperator(\n",
    "        in_keys=[\"state_value\"],\n",
    "        out_keys=[\"state_value\"],\n",
    "    )\n",
    "\n",
    "    return actor_module, value_module\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Configuración del Collector y pérdida PPO (ClipPPOLoss + GAE)\n",
    "\n",
    "# %% [code]\n",
    "# 6.1) Hiperparámetros\n",
    "frames_per_batch   = 1024    # pasos por iteración\n",
    "num_iterations     = 256     # iteraciones (épocas) totales\n",
    "ppo_epoch          = 4       # pasadas sobre cada batch\n",
    "sub_batch_size     = 128     # tamaño de minibatch en PPO\n",
    "gamma              = 0.99\n",
    "lam                = 0.95\n",
    "learning_rate      = 2e-4\n",
    "\n",
    "# 6.2) Crear entornos TorchRL\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env_train = make_torchrl_env(device=device)\n",
    "env_eval  = make_torchrl_env(device=device)\n",
    "\n",
    "# 6.3) Crear Actor y Crítico\n",
    "actor_module, value_module = make_ac_modules(device=device)\n",
    "\n",
    "# 6.4) Establecer exploración “on-policy”\n",
    "set_exploration_type(ExplorationType.RANDOM)\n",
    "\n",
    "# 6.5) ClipPPOLoss y GAE\n",
    "clip_ppo_loss = ClipPPOLoss(\n",
    "    actor=actor_module,\n",
    "    value_network=value_module,\n",
    "    clip_eps=0.2,\n",
    "    value_loss_coeff=0.5,\n",
    "    entropy_coeff=0.01,\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(actor_module.parameters()) + list(value_module.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "gae_module = GAE(\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "# 6.6) Collector síncrono con AutoResetEnv (idéntico a tu notebook)\n",
    "train_collector = SyncDataCollector(\n",
    "    env=AutoResetEnv(env_train),\n",
    "    policy=actor_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=None,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Rutina de entrenamiento + evaluación con tqdm y plot(logs)\n",
    "\n",
    "# %% [code]\n",
    "# 7.1) Logs para graficar\n",
    "logs = {\n",
    "    \"train_reward\": [],\n",
    "    \"eval_reward\":  []\n",
    "}\n",
    "num_eval_episodes = 3  # episodios de evaluación por iteración\n",
    "\n",
    "pbar = tqdm(range(num_iterations), desc=\"PPO Entrenando\", unit=\"iter\")\n",
    "for iter_idx in pbar:\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (1) RECOLECTAR frames_per_batch pasos (rollout on-policy)\n",
    "    # —————————————————————————————————————————————————\n",
    "    experience = next(train_collector)\n",
    "    # experience: TensorDict con keys como:\n",
    "    #   \"pixels\", \"action\", \"log_prob\", \"state_value\", \"next_pixels\", \"reward\", \"done\", \"rollout_info\", ...\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (2) CALCULAR VENTAJAS Y RETORNOS (GAE)\n",
    "    # —————————————————————————————————————————————————\n",
    "    _ = gae_module(experience)\n",
    "    # Ahora `experience` contiene \"advantage\" y \"return\"\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (3) OPTIMIZAR CON ClipPPOLoss\n",
    "    # —————————————————————————————————————————————————\n",
    "    _ = clip_ppo_loss(\n",
    "        experience,\n",
    "        optimizer=optimizer,\n",
    "        ppo_epoch=ppo_epoch,\n",
    "        mini_batch_size=sub_batch_size,\n",
    "    )\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (4) CALCULAR avg_train_reward\n",
    "    # —————————————————————————————————————————————————\n",
    "    train_episode_rewards = experience.get((\"rollout_info\", \"reward_sum\")).cpu().numpy()\n",
    "    avg_train_reward = float(np.mean(train_episode_rewards))\n",
    "    logs[\"train_reward\"].append(avg_train_reward)\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (5) EVALUACIÓN (política determinista)\n",
    "    # —————————————————————————————————————————————————\n",
    "    set_exploration_type(ExplorationType.MODE)  # fuerza argmax en ProbabilisticActor\n",
    "    eval_episode_rewards = []\n",
    "    for _ in range(num_eval_episodes):\n",
    "        td = env_eval.reset()  # Tensordict con “pixels”\n",
    "        done = False\n",
    "        ep_rew = 0.0\n",
    "        while not done:\n",
    "            obs_td = td.clone().to(device)\n",
    "            out = actor_module(obs_td)                 # saca acción determinista\n",
    "            action = out.get(\"action\").item()\n",
    "            td, reward, term, trunc, _ = env_eval.step(action)\n",
    "            done = bool(term or trunc)\n",
    "            ep_rew += reward\n",
    "        eval_episode_rewards.append(ep_rew)\n",
    "    avg_eval_reward = float(np.mean(eval_episode_rewards))\n",
    "    logs[\"eval_reward\"].append(avg_eval_reward)\n",
    "    set_exploration_type(ExplorationType.RANDOM)  # de vuelta a modo entrenamiento\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # (6) Actualizar tqdm y graficar\n",
    "    # —————————————————————————————————————————————————\n",
    "    pbar.set_postfix({\n",
    "        \"avg_train\": f\"{avg_train_reward:.1f}\",\n",
    "        \"avg_eval\":  f\"{avg_eval_reward:.1f}\"\n",
    "    })\n",
    "    plot(logs)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Guardar modelo final\n",
    "#\n",
    "# Si quieres salvar el checkpoint del actor y el crítico:\n",
    "# ```python\n",
    "# save_name = f\"ppo_cr_{uuid.uuid4().hex[:6]}.pt\"\n",
    "# checkpoint = {\n",
    "#     \"actor_state_dict\": actor_module.state_dict(),\n",
    "#     \"value_state_dict\": value_module.state_dict(),\n",
    "# }\n",
    "# torch.save(checkpoint, save_name)\n",
    "# print(\"Modelo guardado en\", save_name)\n",
    "# ```\n",
    "#\n",
    "# ———————————————————————————————————————————————————————————————————————————————  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
