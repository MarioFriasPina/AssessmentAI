{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07e5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y swig python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # PPO en CarRacing-v3 con TorchRL (gráficas + tqdm)\n",
    "#\n",
    "# Este Notebook enseña cómo usar PPO de TorchRL para entrenar CarRacing-v3 (con acciones discretas), e incluye gráficos en vivo (como en el IPYNB que enviaste) y una barra de progreso (`tqdm`) que muestra el avance por iteración, junto con “Avg Reward (Train)” y “Avg Reward (Eval)”.\n",
    "#\n",
    "# **Dependencias**:\n",
    "# ```\n",
    "# pip install torch torchvision torchrl gymnasium[box2d] tqdm matplotlib\n",
    "# ```\n",
    "#\n",
    "# ———————————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Importaciones y utilidades de graficado\n",
    "#\n",
    "# — En esta sección importamos todo lo necesario: Torch, TorchRL, Gym, Matplotlib, tqdm, y definimos la función `plot(logs)` (idéntica a la de tu notebook) que reconstruye la figura en cada iteración.\n",
    "\n",
    "# %% [code]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.transforms import ToTensorImage, Compose\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "\n",
    "from torchrl.modules import SafeModule, ProbabilisticActor, ValueOperator, TanhNormal\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# — Función de graficado (idéntica a la del IPYNB de ejemplo) —\n",
    "def plot(logs):\n",
    "    \"\"\"\n",
    "    Recibe un dict `logs` con listas en:\n",
    "      logs[\"train_reward\"] = [r1, r2, r3, ...]\n",
    "      logs[\"eval_reward\"]  = [e1, e2, e3, ...]\n",
    "    y dibuja dos subplots: \n",
    "      · Avg Reward (Train)\n",
    "      · Avg Reward (Eval)\n",
    "    Llamar esta función tras actualizar `logs` para redibujar la gráfica en línea.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\"Avg Reward (Train)\", \"Avg Reward (Eval)\"]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"], \"blue\"),\n",
    "        (logs[\"eval_reward\"],  \"green\"),\n",
    "    ]\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Wrapper discreto para CarRacing-v3\n",
    "#\n",
    "# El entorno original de CarRacing usa un espacio continuo de 3 dimensión (`[steer, gas, brake]`), pero queremos usar PPO discreto con 5 acciones:\n",
    "#\n",
    "# 1. `0`: no-op            → `[ 0.0,  0.0,  0.0 ]`  \n",
    "# 2. `1`: acelerar        → `[ 0.0,  1.0,  0.0 ]`  \n",
    "# 3. `2`: frenar          → `[ 0.0,  0.0,  0.8 ]`  \n",
    "# 4. `3`: girar izquierda → `[−1.0,  0.0,  0.0 ]`  \n",
    "# 5. `4`: girar derecha   → `[ 1.0,  0.0,  0.0 ]`\n",
    "#\n",
    "# Este wrapper convierte un entero discreto (0–4) en el vector contínuo que CarRacing espera.\n",
    "\n",
    "# %% [code]\n",
    "class DiscreteCarRacingWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Convierte la acción discreta en un vector [steer, gas, brake].\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super().__init__(env)\n",
    "        self.discrete_actions = [\n",
    "            np.array([ 0.0,  0.0,  0.0 ], dtype=np.float32),  # 0: no-op\n",
    "            np.array([ 0.0,  1.0,  0.0 ], dtype=np.float32),  # 1: acelerar\n",
    "            np.array([ 0.0,  0.0,  0.8 ], dtype=np.float32),  # 2: frenar\n",
    "            np.array([-1.0,  0.0,  0.0 ], dtype=np.float32),  # 3: girar izquierda\n",
    "            np.array([ 1.0,  0.0,  0.0 ], dtype=np.float32),  # 4: girar derecha\n",
    "        ]\n",
    "        self.action_space = gym.spaces.Discrete(len(self.discrete_actions))\n",
    "\n",
    "    def action(self, action_discrete: int) -> np.ndarray:\n",
    "        return self.discrete_actions[int(action_discrete)]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Crear entornos TorchRL\n",
    "#\n",
    "# Para PPO necesitamos:\n",
    "# 1. **entorno de entrenamiento** (sin render, con transform).  \n",
    "# 2. **entorno de evaluación** (también sin render, pero lo usaremos solo para medir rendimiento).  \n",
    "#\n",
    "# Usamos `GymEnv` con `from_pixels=True` + `ToTensorImage()` para que cada paso regrese un Tensordict con:\n",
    "# - `\"pixels\"`: Tensor `[3×96×96]` en `[0,1]`.  \n",
    "# - `\"action\"`: int escalar en `[0..4]`.  \n",
    "\n",
    "# %% [code]\n",
    "def make_torchrl_env(seed: int = 0, device: str = \"cpu\"):\n",
    "    # 1) Gym base\n",
    "    gym_env = gym.make(\"CarRacing-v3\", render_mode=None)\n",
    "    # 2) Wrapper discreto\n",
    "    gym_env = DiscreteCarRacingWrapper(gym_env)\n",
    "    gym_env.seed(seed)\n",
    "    # 3) GymEnv (TorchRL) con frames en Tensor\n",
    "    trl_env = GymEnv(\n",
    "        env=gym_env,\n",
    "        from_pixels=True,\n",
    "        pixels_only=True,\n",
    "        device=device\n",
    "    )\n",
    "    # 4) Transform (uint8 HWC → float32 CHW en [0,1])\n",
    "    trl_env.set_transform(Compose(ToTensorImage()))\n",
    "    return trl_env\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Definir Actor-Crítico con SafeModule + ProbabilisticActor + ValueOperator\n",
    "#\n",
    "# - La red convolucional es idéntica a la del ejemplo: 3 → 32 conv8×8/stride 4 → 64 conv4×4/stride 2 → 64 conv3×3/stride 1 → Flatten → FC 512.  \n",
    "# - **Policy head**: saca logits sobre 5 acciones discretas.  \n",
    "# - **Value head**: saca un escalar (valor del estado).  \n",
    "#\n",
    "# Con TorchRL envolvemos así:\n",
    "# 1. `SafeModule` para el feature extractor y dos cabezas separadas (`policy_head`, `value_head`).  \n",
    "# 2. `ProbabilisticActor` para convertir “logits” en una `torch.distributions.Categorical`.  \n",
    "# 3. `ValueOperator` para exponer la salida de valor.  \n",
    "\n",
    "# %% [code]\n",
    "# 4.1) Red CNN base\n",
    "class CNNBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)   # → [32,23,23]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  # → [64,10,10]\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  # → [64,8,8]\n",
    "        self.flatten_dim = 64 * 8 * 8\n",
    "        self.fc = nn.Linear(self.flatten_dim, 512)\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(obs_image))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.flatten_dim)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x  # → [B,512]\n",
    "\n",
    "# 4.2) Construcción del módulo completo con dos salidas\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, num_actions: int = 5):\n",
    "        super().__init__()\n",
    "        self.body = CNNBody()\n",
    "        # Cabeza de política: produce logits [B,5]\n",
    "        self.policy_head = nn.Linear(512, num_actions)\n",
    "        # Cabeza de valor: produce valor escalar [B,1]\n",
    "        self.value_head  = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, obs_image: torch.Tensor):\n",
    "        features = self.body(obs_image)                # [B,512]\n",
    "        logits   = self.policy_head(features)          # [B,5]\n",
    "        value    = self.value_head(features).squeeze(-1)  # [B]\n",
    "        return logits, value\n",
    "\n",
    "# 4.3) Armado de SafeModule + ProbabilisticActor + ValueOperator\n",
    "def make_ac_modules(device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - actor_module: ProbabilisticActor que espera 'pixels' y saca 'action', 'log_prob'\n",
    "      - value_module: ValueOperator que espera 'pixels' y saca 'value'\n",
    "    \"\"\"\n",
    "    base_net = ActorCriticNet(num_actions=5).to(device)\n",
    "    # Para la política:\n",
    "    actor_sm = SafeModule(\n",
    "        base_net, \n",
    "        in_keys=[\"pixels\"],            # recibe tensor 'pixels'\n",
    "        out_keys=[\"logits\", \"value\"],  # reuse: esta forward devuelve (logits, value)\n",
    "        function=lambda m, x: m(x)     # m(x) → (logits,value)\n",
    "    )\n",
    "    # De los logits salimos con Categorical:\n",
    "    actor_module = ProbabilisticActor(\n",
    "        actor_sm,\n",
    "        in_keys=[\"logits\"],\n",
    "        out_keys=[\"action\", \"log_prob\"],\n",
    "        distribution_class=Categorical,\n",
    "        distribution_kwargs={},       # Categorical(logits=logits)\n",
    "        return_log_prob=True,\n",
    "        default_interaction_mode=\"random\",  # en train samplea, en eval podemos hacer mode=\"mode\"\n",
    "    )\n",
    "\n",
    "    # Para la cabeza de valor:\n",
    "    value_sm = SafeModule(\n",
    "        base_net, \n",
    "        in_keys=[\"pixels\"],\n",
    "        out_keys=[\"value\"], \n",
    "        function=lambda m, x: m(x)[1].unsqueeze(-1)  \n",
    "        # m(x)[1] ya es [B], le agrego dim -1 para que ValueOperator espere [B,1]\n",
    "    )\n",
    "    value_module = ValueOperator(\n",
    "        in_keys=[\"value\"],   # recibe 'value'\n",
    "        out_keys=[\"state_value\"],\n",
    "    )\n",
    "\n",
    "    return actor_module, value_module\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Configuración del Collector y pérdida PPO (ClipPPOLoss + GAE)\n",
    "#\n",
    "# - `frames_per_batch`: cuántos pasos (frames) recolectamos antes de hacer update de PPO.  \n",
    "# - `num_iterations`: cuántas veces repetimos ese ciclo recolectar→optimizar.  \n",
    "# - Internamente, ClipPPOLoss calculará la pérdida de crítica (MSE), la pérdida de política con clipping, y entropía.  \n",
    "# - Para GAE definimos un objeto `GAE(gamma, lam)` para generar las ventajas.  \n",
    "\n",
    "# %% [code]\n",
    "# 5.1) Hiperparámetros\n",
    "frames_per_batch   = 1024    # pasos a recolectar por iteración\n",
    "num_iterations     = 256     # cuántas iteraciones (epocas) de PPO\n",
    "ppo_epoch          = 4       # cuántas pasadas sobre cada batch de datos\n",
    "sub_batch_size     = 128     # minibatch para optimizador\n",
    "gamma              = 0.99\n",
    "lam                = 0.95\n",
    "learning_rate      = 2e-4\n",
    "\n",
    "# 5.2) Crear entornos\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env_train = make_torchrl_env(seed=123, device=device)\n",
    "env_eval  = make_torchrl_env(seed=456, device=device)\n",
    "\n",
    "# 5.3) Crear módulos Actor y Critic\n",
    "actor_module, value_module = make_ac_modules(device=device)\n",
    "\n",
    "# 5.4) Establecer exploración por defecto (importante para PPO “on-policy”)\n",
    "#     TorchRL usa ExplorationType.RANDOM en training y EXPLORATION.EVAL en evaluación.\n",
    "set_exploration_type(ExplorationType.RANDOM)\n",
    "\n",
    "# 5.5) ClipPPOLoss y GAE\n",
    "#    - td_steps=frames_per_batch: cuántos pasos en el rollout\n",
    "#    - clip_eps=0.2: epsilon de clipping\n",
    "#    - value_loss_coeff=0.5, entropy_coeff=0.01  (valores típicos)\n",
    "clip_ppo_loss = ClipPPOLoss(\n",
    "    actor=actor_module,\n",
    "    value_network=value_module,\n",
    "    clip_eps=0.2,\n",
    "    value_loss_coeff=0.5,\n",
    "    entropy_coeff=0.01,\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "\n",
    "# 5.6) Optimizadores (uno solo que abarque params de actor y critic)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(actor_module.parameters()) + list(value_module.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# 5.7) GAE (para calcular ventajas dentro de ClipPPOLoss)\n",
    "gae_module = GAE(\n",
    "    gamma=gamma,\n",
    "    lambda_=lam,\n",
    "    reduction=\"mean\"\n",
    ")\n",
    "\n",
    "# 5.8) Collector síncrono (auto‐reset) para recolección de transiciones\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.envs import AutoResetEnv\n",
    "\n",
    "# AutoResetEnv se encarga de llamar reset() internamente\n",
    "train_collector = SyncDataCollector(\n",
    "    env=AutoResetEnv(env_train),\n",
    "    policy=actor_module,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=None,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# ———————————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Rutina de entrenamiento + evaluación con tqdm y plot(logs)\n",
    "#\n",
    "# En cada iteración (de `0` a `num_iterations-1`):\n",
    "# 1. Recolectar `frames_per_batch` pasos con `train_collector`.  \n",
    "# 2. Obtener un Tensordict con las transiciones:  \n",
    "#    - `\"pixels\"`, `\"action\"`, `\"log_prob\"`, `\"next_pixels\"`, `\"reward\"`, `\"done\"`, `\"state_value\"`, etc.  \n",
    "# 3. Calcular ventajas y retornos con `gae_module`.  \n",
    "# 4. Llamar a `clip_ppo_loss` sobre ese Tensordict para optimizar (usa `ppo_epoch` y `sub_batch_size`).  \n",
    "# 5. Medir `avg_train_reward` de esa iteración.  \n",
    "# 6. Correr `eval_eps` episodios en `env_eval` para `avg_eval_reward`.  \n",
    "# 7. Actualizar la barra de progreso, guardar métricas en `logs`, y llamar a `plot(logs)`.  \n",
    "\n",
    "# %% [code]\n",
    "# 6.1) Variables para logs\n",
    "logs = {\n",
    "    \"train_reward\": [],\n",
    "    \"eval_reward\":  []\n",
    "}\n",
    "\n",
    "num_eval_episodes = 3  # cuántos episodios de evaluación por iteración\n",
    "\n",
    "# Barra de progreso\n",
    "pbar = tqdm(range(num_iterations), desc=\"PPO Entrenando\", unit=\"iter\")\n",
    "\n",
    "for iter_idx in pbar:\n",
    "    # —————————————————————————————————————————————————\n",
    "    # 1) RECOLECTAR frames_per_batch pasos (rollout on-policy)\n",
    "    # —————————————————————————————————————————————————\n",
    "    # `experience` es un TensorDict con keys:\n",
    "    #   \"pixels\", \"action\", \"log_prob\", \"state_value\", \"next_pixels\", \"reward\", \"done\", \"truncated\", ...\n",
    "    experience = next(train_collector)\n",
    "\n",
    "    # 2) CALCULAR VENTAJAS Y RETORNOS (GAE)\n",
    "    # ClipPPOLoss internamente requiere que le pasemos un td con:\n",
    "    #  - \"advantage\": ventaja por paso\n",
    "    #  - \"return\": valor objetivo (retorno descontado)\n",
    "    #  - \"state_value\": valor que dio la red en cada paso\n",
    "    #  - \"log_prob\": log prob original\n",
    "    #  - \"action\": acción tomada\n",
    "    #  - \"reward\", \"done\": info del entorno\n",
    "    #  - \"pixels\", \"next_pixels\": transiciones visuales\n",
    "    #  → GAE rellena \"advantage\" y \"return\" en el mismo TensorDict.\n",
    "    loss_info = gae_module(experience)\n",
    "    # Ahora `experience` contiene \"advantage\" y \"return\"\n",
    "\n",
    "    # 3) OPTIMIZAR CON ClipPPOLoss\n",
    "    #    Le pasamos `experience`, `optimizer`, `ppo_epoch`, `sub_batch_size`\n",
    "    loss_ppo = clip_ppo_loss(\n",
    "        experience,\n",
    "        optimizer=optimizer,\n",
    "        ppo_epoch=ppo_epoch,\n",
    "        mini_batch_size=sub_batch_size,\n",
    "    )\n",
    "\n",
    "    # 4) CALCULAR avg_train_reward para este batch \n",
    "    #    (recompensa total promedio sobre cada episodio dentro de este rollout).\n",
    "    #    TorchRL almacena en \"rollout_info\": un tensor con shape [num_episodios_en_batch]\n",
    "    #    con la suma de rewards por episodio.\n",
    "    train_episode_rewards = experience.get((\"rollout_info\", \"reward_sum\")).cpu().numpy()\n",
    "    avg_train_reward = float(np.mean(train_episode_rewards))\n",
    "    logs[\"train_reward\"].append(avg_train_reward)\n",
    "\n",
    "    # —————————————————————————————————————————————————\n",
    "    # 5) EVALUACIÓN (política determinista)\n",
    "    # —————————————————————————————————————————————————\n",
    "    # Para eval no queremos muestreo aleatorio → cambiamos a modo eval\n",
    "    set_exploration_type(ExplorationType.MODE)  # fuerza argmax en ProbabilisticActor\n",
    "    eval_episode_rewards = []\n",
    "    for _ in range(num_eval_episodes):\n",
    "        td = env_eval.reset()  # td contiene \"pixels\"\n",
    "        done = False\n",
    "        ep_rew = 0.0\n",
    "        while not done:\n",
    "            # Creamos Tensordict de entrada manualmente\n",
    "            obs_td = td.clone().to(device)\n",
    "            # Solo necesitamos \"pixels\" para la política\n",
    "            out = actor_module(obs_td)\n",
    "            action = out.get(\"action\").item()\n",
    "            td, reward, term, trunc, _ = env_eval.step(action)\n",
    "            done = bool(term or trunc)\n",
    "            ep_rew += reward\n",
    "        eval_episode_rewards.append(ep_rew)\n",
    "    avg_eval_reward = float(np.mean(eval_episode_rewards))\n",
    "    logs[\"eval_reward\"].append(avg_eval_reward)\n",
    "\n",
    "    # → Volvemos a modo entrenamiento (“random” sampling)\n",
    "    set_exploration_type(ExplorationType.RANDOM)\n",
    "\n",
    "    # 6) ACTUALIZAR tqdm Y GRAficar\n",
    "    pbar.set_postfix({\n",
    "        \"avg_train\": f\"{avg_train_reward:.1f}\",\n",
    "        \"avg_eval\":  f\"{avg_eval_reward:.1f}\"\n",
    "    })\n",
    "    plot(logs)\n",
    "\n",
    "# Cerrar barra\n",
    "pbar.close()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Guardar modelo final\n",
    "# \n",
    "# Si quieres salvar el checkpoint del actor y el crítico, puedes hacer:\n",
    "# ```python\n",
    "# save_name = f\"ppo_cr_{uuid.uuid4().hex[:6]}.pt\"\n",
    "# checkpoint = {\n",
    "#     \"actor_state_dict\": actor_module.state_dict(),\n",
    "#     \"value_state_dict\": value_module.state_dict(),\n",
    "# }\n",
    "# torch.save(checkpoint, save_name)\n",
    "# print(\"Modelo guardado en\", save_name)\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Observaciones y consejos finales\n",
    "#\n",
    "# 1. **Hyperparámetros**  \n",
    "#    - `frames_per_batch` ≈ 1024–4096 (depende de tu GPU/CPU).  \n",
    "#    - `num_iterations` = 200–300 (si quieres entrenar más tiempo, subirlo).  \n",
    "#    - `ppo_epoch` = 3–6, `sub_batch_size` = 128.  \n",
    "#    - `gamma` = 0.99, `lam` = 0.95, `clip_eps` = 0.2.  \n",
    "#    - `learning_rate` = 2 × 10⁻⁴.  \n",
    "#\n",
    "# 2. **Funciones de TorchRL**  \n",
    "#    - `ClipPPOLoss` ya engloba:  \n",
    "#       • Cálculo del ratio πθ/πθ₀ y el clamp.  \n",
    "#       • Loss de valor (MSE) con coeficiente `value_loss_coeff`.  \n",
    "#       • Término de entropía (`entropy_coeff`).  \n",
    "#       • Normalización de ventajas si `normalize_advantage=True` (por defecto).  \n",
    "#    - `GAE` crea las llaves `\"advantage\"` y `\"return\"` dentro del TensorDict.  \n",
    "#    - `SyncDataCollector` con `frames_per_batch` devuelve un TensorDict con, entre otras keys:  \n",
    "#       • `\"rollout_info\",\"reward_sum\"`: array con la suma de recompensas por episodio.  \n",
    "#\n",
    "# 3. **Gráficas**  \n",
    "#    - Cada iteración `plot(logs)` reconstruye la figura desde cero (con `clear_output(wait=True)` y `display(fig)`), de modo que verás en vivo cómo se actualizan las curvas de “Avg Reward (Train)” y “Avg Reward (Eval)”.  \n",
    "#\n",
    "# 4. **Exploración**  \n",
    "#    - Usamos `set_exploration_type(ExplorationType.RANDOM)` durante el entrenamiento para que `ProbabilisticActor` samplee acciones.  \n",
    "#    - Para evaluación, cambiamos a `ExplorationType.MODE` para usar la acción `argmax`.  \n",
    "#\n",
    "# 5. **Ajustes**  \n",
    "#    - Si deseas ver el entorno “real” en evaluación, podrías crear un Gym env aparte con `render_mode=\"human\"` y dibujar algunos episodios.  \n",
    "#    - Para guardarlo periódicamente, inserta un bloque dentro del bucle `if iter_idx % 10 == 0: torch.save(...)`.  \n",
    "#\n",
    "# ———————————————————————————————————————————————————————————————————————————————\n",
    "#\n",
    "# ¡Con esto tienes un pipeline PPO 100 % TorchRL para CarRacing-v3, con gráficos en vivo y barra de progreso!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
