{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3915d7fa",
   "metadata": {},
   "source": [
    "# Car Racing Training\n",
    "\n",
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e287218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y swig python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c0a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e6e9f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecNormalize, VecFrameStack\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f44e0d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Directory where we save the outputs of this training\n",
    "runDir = f'./checkpoints/{uuid.uuid4()}'\n",
    "os.makedirs(runDir, exist_ok=True)\n",
    "\n",
    "# Save the monitor logs into a csv file\n",
    "train_monitor_file = os.path.join(runDir, \"train_monitor.csv\")\n",
    "\n",
    "total_timesteps = 5_000_000 # Training Steps\n",
    "n_eval_episodes = 8         # Eval Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cbb2e",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9479ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapedCarRacing(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # Extract speed from info dict if available (CarRacing provides car speed in info)\n",
    "        speed = info.get(\"speed\", 0.0)\n",
    "        # Add small speed bonus (e.g., +0.1 * speed)\n",
    "        shaped_reward = reward + 0.1 * speed\n",
    "        # If speed < 0.1 (agent is stuck) and reward is low, give a penalty\n",
    "        if speed < 0.1 and reward < 0.1:\n",
    "            shaped_reward -= 0.2\n",
    "        return obs, shaped_reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        return self.env.render(mode)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training environment\n",
    "def make_env(env):\n",
    "    env = ShapedCarRacing()\n",
    "    # Convert to grayscale → (96×96×1)\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    return env\n",
    "\n",
    "# Create 8 parallel envs\n",
    "vec_env = DummyVecEnv([make_env for _ in range(8)])\n",
    "# Keep monitor logs for each sub‐env\n",
    "vec_env = VecMonitor(vec_env, filename=train_monitor_file)\n",
    "# Normalize observations and (optionally) rewards\n",
    "vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "# Stack last 4 grayscale frames → final obs shape: (4×84×84)\n",
    "#vec_env = VecFrameStack(vec_env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f3c87",
   "metadata": {},
   "source": [
    "### Eval Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36990189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training VecEnv\n",
    "train_env = vec_env\n",
    "\n",
    "# Create a separate eval env \n",
    "def make_eval_env():\n",
    "    env = ShapedCarRacing()\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    return env\n",
    "\n",
    "eval_vec_env = DummyVecEnv([make_eval_env for _ in range(4)])\n",
    "eval_vec_env = VecNormalize(eval_vec_env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "#eval_vec_env = VecFrameStack(eval_vec_env, n_stack=4)\n",
    "\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=32, min_evals=128, verbose=0)\n",
    "\n",
    "# Now pass eval_vec_env to EvalCallback:\n",
    "eval_callback = EvalCallback(\n",
    "    eval_vec_env,\n",
    "    best_model_save_path=runDir,\n",
    "    log_path=runDir,\n",
    "    n_eval_episodes=8,       # as you had configured\n",
    "    eval_freq=4096,          # e.g., after every 4096 steps across 8 envs\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    callback_after_eval=stop_train_callback,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Model\n",
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=vec_env,\n",
    "    tensorboard_log=runDir,\n",
    "    n_steps=256,\n",
    "    learning_rate=2.5e-4,\n",
    "    batch_size=64,\n",
    "    n_epochs=8,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.005,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# tensorboard --logdir AI/checkpoints/\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True, callback=eval_callback)\n",
    "\n",
    "# Save the model\n",
    "model_path = f\"{runDir}/final_ppo_carracing\"\n",
    "model.save(model_path)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c638907",
   "metadata": {},
   "source": [
    "### Training graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccce764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_monitor_file, skiprows=1)\n",
    "rewards = df[\"r\"].values\n",
    "episodes = np.arange(1, len(rewards) + 1)\n",
    "\n",
    "# 50-episode moving average\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    mov_avg = np.convolve(rewards, np.ones(window)/window, mode=\"valid\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episodes, rewards, label=\"Per‐Episode Reward\")\n",
    "plt.plot(episodes[window-1:], mov_avg, label=f\"MA ({window})\", linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"CarRacing Training Rewards After Modifications\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11482c9b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a30998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last model\n",
    "def make_eval_env():\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"human\")\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    return env\n",
    "\n",
    "eval_env = make_eval_env()\n",
    "model = PPO.load(model_path, env=eval_env)\n",
    "\n",
    "# Manual evaluation loop\n",
    "rewards_eval = []\n",
    "for epi in range(n_eval_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_r = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, r, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_r += r\n",
    "\n",
    "    rewards_eval.append(total_r)\n",
    "\n",
    "rewards_eval = np.array(rewards_eval)\n",
    "episodes_eval = np.arange(1, n_eval_episodes + 1)\n",
    "mean_eval = rewards_eval.mean()\n",
    "std_eval = rewards_eval.std()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(episodes_eval, rewards_eval, '-o', color=\"tab:green\",\n",
    "         label=\"Episode rewards\")\n",
    "plt.axhline(mean_eval, color=\"tab:red\", linestyle=\"--\",\n",
    "            label=f\"Average: {mean_eval:.2f} ± {std_eval:.2f}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Rewards\")\n",
    "plt.title(f\"CarRacing: Evaluation in {n_eval_episodes} episodes\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Eval → Average Reward: {mean_eval:.2f}; Standard Deviation: {std_eval:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
