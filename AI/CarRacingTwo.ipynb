{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt-get install -y swig python3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a30998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURACIÓN DE RUTAS Y PARÁMETROS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "logs_dir = \"logs\"\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "train_monitor_file = os.path.join(logs_dir, \"train_monitor.csv\")\n",
    "\n",
    "total_timesteps = int(2e5)\n",
    "n_eval_episodes = 30\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1. ENTRENAMIENTO CON PPO\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1.1. Creamos y envolvemos el env\n",
    "env_train = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "env_train = Monitor(env_train, filename=train_monitor_file)\n",
    "\n",
    "# 1.2. Instanciamos PPO\n",
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=env_train,\n",
    "    verbose=1,\n",
    "    tensorboard_log=None,\n",
    "    n_steps=2048,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2,\n",
    "    gae_lambda=0.95,\n",
    ")\n",
    "\n",
    "# 1.3. Entrenamos\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "# 1.4. Guardamos\n",
    "model_path = \"ppo_carracing\"\n",
    "model.save(model_path)\n",
    "del model\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2. GRAFICA DE RECOMPENSA EN ENTRENAMIENTO\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2.1. Leer CSV generado por Monitor\n",
    "df_train = pd.read_csv(train_monitor_file, skiprows=1)\n",
    "rewards_train = df_train[\"r\"].values\n",
    "episodes_train = np.arange(1, len(rewards_train) + 1)\n",
    "\n",
    "# 2.2. (Opcional) Promedio móvil\n",
    "window = 50\n",
    "if len(rewards_train) >= window:\n",
    "    mov_avg = np.convolve(rewards_train, np.ones(window) / window, mode=\"valid\")\n",
    "else:\n",
    "    mov_avg = None\n",
    "\n",
    "# 2.3. Graficar\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episodes_train, rewards_train, label=\"Recompensa por episodio\", color=\"tab:blue\")\n",
    "if mov_avg is not None:\n",
    "    plt.plot(episodes_train[window-1:], mov_avg,\n",
    "             label=f\"Promedio móvil ({window} ep)\", color=\"tab:orange\")\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Recompensa total\")\n",
    "plt.title(\"CarRacing-v3: Recompensa en Entrenamiento\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3. EVALUACIÓN Y SU GRÁFICA\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3.1. Recargamos el modelo en un entorno nuevo\n",
    "eval_env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "model = PPO.load(model_path, env=eval_env)\n",
    "\n",
    "# 3.2. Loop de evaluación manual\n",
    "rewards_eval = []\n",
    "for epi in range(n_eval_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_r = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, r, terminated, truncated, info = eval_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_r += r\n",
    "\n",
    "    rewards_eval.append(total_r)\n",
    "\n",
    "rewards_eval = np.array(rewards_eval)\n",
    "episodes_eval = np.arange(1, n_eval_episodes + 1)\n",
    "mean_eval = rewards_eval.mean()\n",
    "std_eval = rewards_eval.std()\n",
    "\n",
    "# 3.3. Graficar resultados de eval\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(episodes_eval, rewards_eval, '-o', color=\"tab:green\",\n",
    "         label=\"Recompensa episodio\")\n",
    "plt.axhline(mean_eval, color=\"tab:red\", linestyle=\"--\",\n",
    "            label=f\"Promedio: {mean_eval:.2f} ± {std_eval:.2f}\")\n",
    "plt.xlabel(\"Episodio de evaluación\")\n",
    "plt.ylabel(\"Recompensa total\")\n",
    "plt.title(f\"CarRacing-v3: Evaluación en {n_eval_episodes} episodios\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Eval → Recompensa media: {mean_eval:.2f}; Desviación: {std_eval:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
