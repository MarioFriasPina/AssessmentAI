{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b813e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r train_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960300",
   "metadata": {},
   "source": [
    "## PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch import multiprocessing\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Information\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# TorchRL\n",
    "from torchrl.envs import (Compose, DoubleToFloat, ObservationNorm, StepCounter, TransformedEnv)\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
    "from torchrl.modules import ProbabilisticActor, SafeModule, ValueOperator, TanhNormal, ActorValueOperator, NormalParamExtractor\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "\n",
    "# Environment\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "# Model hyper-parameters\n",
    "lr = 3e-4\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Collector hyper-parameters\n",
    "frames_per_batch = 128 # 1024 # number of frames collected per batch\n",
    "total_frames = 65536  # total number of frames to collect\n",
    "\n",
    "# PPO hyper-parameters\n",
    "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\n",
    "num_epochs = 8 # optimization steps per batch of data collected\n",
    "clip_epsilon = ( 0.2 )  # clip value for PPO loss\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "entropy_eps = 1e-4\n",
    "\n",
    "# Checkpoint saving parameters\n",
    "checkpoint_interval = 8\n",
    "filename = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af235e9",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b586b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"CarRacing-v3\", continuous=False, render_mode=\"rgb_array\", device=device)\n",
    "\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)\n",
    "print(\"input_spec:\", env.input_spec)\n",
    "print(\"action_spec (as defined by input_spec):\", env.action_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b01750",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b51dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros((1, 3, 96, 96), device=device)\n",
    "            n_flat = self.features(dummy).shape[1]\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(n_flat, 512, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions, device=device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dtype == torch.uint8:\n",
    "            x = x.float() / 255.0\n",
    "        # If input is NHWC, convert to NCHW\n",
    "        if x.ndim == 4 and x.shape[-1] == 3:\n",
    "            x = x.permute(0, 3, 1, 2)  # (B, H, W, C) -> (B, C, H, W)\n",
    "        elif x.ndim == 3 and x.shape[0] == 3:\n",
    "            pass  # Already (C, H, W)\n",
    "        elif x.ndim == 3 and x.shape[-1] == 3:\n",
    "            x = x.permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Transforms observations to logits using module\n",
    "actor = TensorDictModule(\n",
    "    module=RaceCNN(env.action_spec.shape.numel()),\n",
    "    in_keys=[\"pixels\"],\n",
    "    out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "# \n",
    "actor = ProbabilisticActor(\n",
    "    module=actor,\n",
    "    spec=env.action_spec,\n",
    "    in_keys=[\"logits\"],\n",
    "    distribution_class=Categorical,\n",
    "    return_log_prob=True,\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "value = ValueOperator(\n",
    "    module=RaceCNN(1),\n",
    "    in_keys=[\"pixels\"],\n",
    ")\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env=env,\n",
    "    policy=actor,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    device=device,\n",
    "    replace=False,\n",
    ")\n",
    "\n",
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(), sampler=SamplerWithoutReplacement\n",
    ")\n",
    "\n",
    "advantage_module = GAE(\n",
    "    gamma=gamma, lmbda=lmbda, value_network=value, average_gae=True, device=device,\n",
    ")\n",
    "\n",
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=actor,\n",
    "    critic_network=value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=bool(entropy_eps),\n",
    "    entropy_coef=entropy_eps,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optim, total_frames // frames_per_batch, 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cea7d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654737",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=total_frames)\n",
    "pbar.set_description(\"Training \")\n",
    "\n",
    "logs = defaultdict(list)\n",
    "\n",
    "# # Save model\n",
    "# num_runs = len(os.listdir(\"checkpoints\"))\n",
    "# runDir = f'./checkpoints/{num_runs}'\n",
    "# os.mkdir(runDir)\n",
    "\n",
    "# We get a batch of data from the collector\n",
    "for i, tensordict_data in enumerate(collector):\n",
    "\n",
    "    print(tensordict_data)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        advantage_module(tensordict_data)\n",
    "\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "\n",
    "        # Add the data to the replay buffer\n",
    "        replay_buffer.extend(data_view.cpu())\n",
    "\n",
    "        # Sample sub-batches from the replay buffer\n",
    "        for _ in range(frames_per_batch // sub_batch_size):\n",
    "            subdata = replay_buffer.sample(sub_batch_size)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss_vals = loss_module(subdata.to(device))\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "            \n",
    "            # Backpropagate and optimize\n",
    "            loss_value.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    logs[\"train_reward\"].append( tensordict_data[\"next\", \"reward\"].mean().item() )\n",
    "    #logs[\"train_steps_min\"].append( tensordict_data[\"step_count\"].min().item() )\n",
    "    #logs[\"train_steps_max\"].append( tensordict_data[\"step_count\"].max().item() )\n",
    "\n",
    "    # Execute the policy without exploration\n",
    "    if i % checkpoint_interval == 0:\n",
    "        pbar.set_description(\"Evaluation \")\n",
    "        with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n",
    "            # Run the env with the actors value\n",
    "            eval_rollout = env.rollout(512, actor)\n",
    "\n",
    "            # Save the evaluation data\n",
    "            logs[\"eval_reward\"].append( eval_rollout[\"next\", \"reward\"].mean().item() )\n",
    "            #logs[\"eval_steps\"].append( eval_rollout[\"step_count\"].min().item() )\n",
    "            del eval_rollout\n",
    "\n",
    "            # # Save a checkpoint\n",
    "            # filename = f'{runDir}/{i}.ch'\n",
    "            # checkpoint = {\n",
    "            #     'model_state_dict': actor.module.state_dict(),\n",
    "            # }\n",
    "\n",
    "            # torch.save(checkpoint, filename)\n",
    "\n",
    "    pbar.set_description(\"Training \")\n",
    "\n",
    "    # Update plot data\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Rebuild the figure from scratch\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes = axes.flatten()\n",
    "    titles = [\n",
    "        \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "        #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "    ]\n",
    "    data = [\n",
    "        (logs[\"train_reward\"],   \"blue\"),\n",
    "        (logs[\"eval_reward\"],    \"green\"),\n",
    "        #(logs[\"train_steps_min\"],    \"red\"),\n",
    "        #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "    ]\n",
    "\n",
    "    for ax, title, (y, color) in zip(axes, titles, data):\n",
    "        ax.plot(y, color=color)\n",
    "        ax.set_title(title)\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the new figure\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(tensordict_data.numel())\n",
    "\n",
    "# # Final save of the model\n",
    "# filename = f'{runDir}/_final.ch'\n",
    "# checkpoint = {\n",
    "#     'model_state_dict': actor.module.state_dict(),\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23840d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "if filename == \"\":\n",
    "    num_runs = len(os.listdir(\"checkpoints\"))\n",
    "    filename = f'./checkpoints/{num_runs-1}/_final.ch'\n",
    "    runDir = f'./checkpoints/{num_runs-1}'\n",
    "else:\n",
    "    with open(f'{runDir}/logs.json', 'w') as f:\n",
    "        f.write(json.dumps(logs))\n",
    "\n",
    "l_checkpoint = torch.load(filename)\n",
    "\n",
    "actor.module.state_dict(l_checkpoint[\"model_state_dict\"])\n",
    "\n",
    "env.rollout(512, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{runDir}/logs.json', 'r') as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "axes = axes.flatten()\n",
    "titles = [\n",
    "    \"Avg Reward (Train)\", \"Avg Reward (Eval)\",\n",
    "    #\"Min Steps (Train)\", \"Max Steps (Train)\",\n",
    "]\n",
    "data = [\n",
    "    (logs[\"train_reward\"],   \"blue\"),\n",
    "    (logs[\"eval_reward\"],    \"green\"),\n",
    "    #(logs[\"train_steps_min\"],    \"red\"),\n",
    "    #(logs[\"train_steps_max\"],     \"orange\"),\n",
    "]\n",
    "\n",
    "for ax, title, (y, color) in zip(axes, titles, data):\n",
    "    ax.plot(y, color=color)\n",
    "    ax.set_title(title)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
